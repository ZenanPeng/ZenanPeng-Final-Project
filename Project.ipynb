{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1uAUJGEUzfNj6OsWNAimnYCw7eKaHhMUfU1MTj9YwYw4/edit?usp=sharing), [grading rubric](https://docs.google.com/document/d/1hKuRWqFcIdhOkow3Nljcm7PXzIkoa9c_aHkMKZDxWa0/edit?usp=sharing)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only an outline to help you with your own approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all necessary third-party modules\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "import re\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import math\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need; some have been added for you, and \n",
    "# some you need to fill in\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"data/taxi_zones\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "UBER_CSV = \"data/uber_rides_sample.csv\"\n",
    "WEATHER_CSV_DIR = \"\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d53e24",
   "metadata": {},
   "source": [
    "### Load Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58708809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_zones():\n",
    "    '''\n",
    "    This function is used for loading the taxi_zones.shp file and cleaning the file,\n",
    "    including adding longitude and latitude columns by looking up the polygon,\n",
    "    removing the duplicate LocationID, setting a new index, and removing useless columns\n",
    "    '''\n",
    "    \n",
    "    #load taxi_zones shapefile\n",
    "    taxi_zones_shapefile = gpd.read_file(TAXI_ZONES_SHAPEFILE)\n",
    "    shp = taxi_zones_shapefile.to_crs(4326)\n",
    "    \n",
    "    #look up and get longitude column\n",
    "    shp[\"longitude\"]=shp.centroid.x\n",
    "    #look up and get latitude column\n",
    "    shp[\"latitude\"]=shp.centroid.y\n",
    "    \n",
    "    #check if there are any duplicate values in LocationID \n",
    "    if shp.loc[shp['OBJECTID'] != shp['LocationID']]['LocationID'].count() != 0:\n",
    "        #manually correct the duplicate values of LocationID\n",
    "        shp.loc[shp['OBJECTID'] == 57, 'LocationID'] = 57\n",
    "        shp.loc[shp['OBJECTID'] == 104, 'LocationID'] = 104\n",
    "        shp.loc[shp['OBJECTID'] == 105, 'LocationID'] = 105\n",
    "    \n",
    "    #check if the index is 'OBJECTID'\n",
    "    if shp.index.name != 'OBJECTID':\n",
    "        #change the index to 'OBJECTID'\n",
    "        shp = shp.set_index('OBJECTID')\n",
    "    \n",
    "    #only keep useful columns\n",
    "    shp = shp[['LocationID','longitude','latitude']]\n",
    "    \n",
    "    #after cleaning, return the shp file\n",
    "    return shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2479c0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LocationID</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OBJECTID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-74.174000</td>\n",
       "      <td>40.691831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-73.831299</td>\n",
       "      <td>40.616745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-73.847422</td>\n",
       "      <td>40.864474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-73.976968</td>\n",
       "      <td>40.723752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>-74.188484</td>\n",
       "      <td>40.552659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>259</td>\n",
       "      <td>-73.852215</td>\n",
       "      <td>40.897932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>260</td>\n",
       "      <td>-73.906306</td>\n",
       "      <td>40.744235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>261</td>\n",
       "      <td>-74.013023</td>\n",
       "      <td>40.709139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>262</td>\n",
       "      <td>-73.946510</td>\n",
       "      <td>40.775932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>263</td>\n",
       "      <td>-73.951010</td>\n",
       "      <td>40.778766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>263 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          LocationID  longitude   latitude\n",
       "OBJECTID                                  \n",
       "1                  1 -74.174000  40.691831\n",
       "2                  2 -73.831299  40.616745\n",
       "3                  3 -73.847422  40.864474\n",
       "4                  4 -73.976968  40.723752\n",
       "5                  5 -74.188484  40.552659\n",
       "...              ...        ...        ...\n",
       "259              259 -73.852215  40.897932\n",
       "260              260 -73.906306  40.744235\n",
       "261              261 -74.013023  40.709139\n",
       "262              262 -73.946510  40.775932\n",
       "263              263 -73.951010  40.778766\n",
       "\n",
       "[263 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_and_clean_taxi_zones()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_with_coords(from_coord, to_coord):\n",
    "    \n",
    "    #use haversine formula to calculate the distance based on latitude and longitude\n",
    "    \n",
    "    #the earth radius in km\n",
    "    R = 6371.0\n",
    "    \n",
    "    pickup_lat = math.radians(from_coord['pickup_latitude'])\n",
    "    pickup_lon = math.radians(from_coord['pickup_longitude'])\n",
    "    dropoff_lat = math.radians(to_coord['dropoff_latitude'])\n",
    "    dropoff_lon = math.radians(to_coord['dropoff_longitude'])\n",
    "    \n",
    "    diff_lon = dropoff_lon - pickup_lon\n",
    "    diff_lat = dropoff_lat - pickup_lat\n",
    "    \n",
    "    a = math.sin(diff_lat/2)**2 + math.cos(pickup_lat) * math.cos(dropoff_lat) * math.sin(diff_lon/2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "    \n",
    "    #return the calculated distance based on latitude and longitude\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d6abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_column(df):\n",
    "    \n",
    "    #add one column call distance to the dataframe\n",
    "    df['distance'] = df.apply(lambda x: calculate_distance_with_coords(\n",
    "                            x[['pickup_latitude','pickup_longitude']], \n",
    "                            x[['dropoff_latitude','dropoff_longitude']]), axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Process Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e78b85ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_yellow_taxi_urls(TAXI_URL):\n",
    "    \"\"\"\n",
    "    This function, get_parquet_files(), is used for collecting all urls of \n",
    "    yellow taxi data from 2009-01 to 2015-06 and save them into a list.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = requests.get(TAXI_URL)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    #write a regular expression to help pull out the desired links for Yellow Taxi Parquet files\n",
    "    pattern = r\"yellow_tripdata_(2009-(0[1-9]|1[0-2])|201[0-4]-(0[1-9]|1[0-2])|2015-(0[1-6]))\"\n",
    "    parquet_links_list = []\n",
    "    \n",
    "    for url in [a['href'] for a in soup.find_all('a')]:\n",
    "        if re.search(pattern, url):\n",
    "            parquet_links_list.append(url)\n",
    "            \n",
    "    return parquet_links_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e668788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-12.parquet']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_yellow_taxi_urls(TAXI_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month(taxi_data_url):\n",
    "    \n",
    "    #give an unique file name by slicing the corresponding month \n",
    "    filename = 'data/' + taxi_data_url.split('/')[-1] \n",
    "    \n",
    "    #check if the file is already downloaded to the current path\n",
    "    if not os.path.exists(filename):\n",
    "        response = requests.get(taxi_data_url, stream=True)\n",
    "        #download the monthly parquet file from correspond taxi url\n",
    "        with open(filename, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024): \n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "    \n",
    "    #clean the parquet file for corresponding month\n",
    "    if '2009' in filename:\n",
    "        \n",
    "        df = pd.read_parquet(filename)\n",
    "        \n",
    "        df = df.dropna(subset={'vendor_name','Trip_Pickup_DateTime','Trip_Dropoff_DateTime','Payment_Type'})\n",
    "        \n",
    "        df = df[(df['Passenger_Count']>0)&(df['Trip_Distance'] > 0)&(df['Fare_Amt'] > 0)&(df['Tip_Amt']>=0)]\n",
    "        \n",
    "        df = df[['Trip_Pickup_DateTime','Start_Lon','Start_Lat','End_Lon','End_Lat','Tip_Amt']]\n",
    "        \n",
    "        df.rename(columns={'Trip_Pickup_DateTime':'pickup_datetime','Start_Lon':'pickup_longitude',\n",
    "                           'Start_Lat':'pickup_latitude','End_Lon':'dropoff_longitude',\n",
    "                           'End_Lat':'dropoff_latitude','Tip_Amt':'tip_amount'}, inplace=True)\n",
    "    \n",
    "    elif '2010' in filename:\n",
    "        \n",
    "        df = pd.read_parquet(filename)\n",
    "        \n",
    "        df = df.dropna(subset={'vendor_id','pickup_datetime','dropoff_datetime','payment_type'})\n",
    "        \n",
    "        df = df[(df['passenger_count']>0)&(df['trip_distance'] > 0)&(df['fare_amount'] > 0)&(df['tip_amount']>=0)]\n",
    "        \n",
    "        df = df[['pickup_datetime','pickup_longitude','pickup_latitude',\n",
    "                 'dropoff_longitude','dropoff_latitude','tip_amount']]\n",
    "    \n",
    "    elif '2011' in filename or '2012' in filename or '2013' in filename or '2014' in filename or '2015' in filename:\n",
    "        \n",
    "        df = pd.read_parquet(filename)\n",
    "        \n",
    "        df = df.dropna(subset={'VendorID','tpep_pickup_datetime','tpep_dropoff_datetime',\n",
    "                               'PULocationID','DOLocationID','payment_type'})\n",
    "        \n",
    "        df = df[(df['passenger_count']>0)&(df['trip_distance'] > 0)&(df['fare_amount'] > 0)&(df['tip_amount']>=0)]\n",
    "        \n",
    "        df = df[['tpep_pickup_datetime','PULocationID','DOLocationID','tip_amount']]\n",
    "        \n",
    "        zones_file = get_and_clean_taxi_zones()\n",
    "        \n",
    "        df = pd.merge(df, zones_file, left_on=\"PULocationID\", right_on=\"LocationID\", how=\"left\")\n",
    "        df = pd.merge(df, zones_file, left_on=\"DOLocationID\", right_on=\"LocationID\", \n",
    "                             suffixes=(\"_PU\", \"_DO\"), how=\"left\")\n",
    "        \n",
    "        df = df[['tpep_pickup_datetime','longitude_PU','latitude_PU',\n",
    "                 'longitude_DO','latitude_DO','tip_amount']]\n",
    "        \n",
    "        df.rename(columns={'tpep_pickup_datetime':'pickup_datetime',\n",
    "                           'longitude_PU':'pickup_longitude','latitude_PU':'pickup_latitude',\n",
    "                           'longitude_DO':'dropoff_longitude','latitude_DO':'dropoff_latitude'},inplace=True)\n",
    "    \n",
    "    #removing all data outside of the New York Box range\n",
    "    \n",
    "    #for pickup_longitude and pickup_latitude\n",
    "    df = df[(df['pickup_longitude']>=NEW_YORK_BOX_COORDS[0][1])\n",
    "            &(df['pickup_longitude']<=NEW_YORK_BOX_COORDS[1][1])\n",
    "            &(df['pickup_latitude']>=NEW_YORK_BOX_COORDS[0][0])\n",
    "            &(df['pickup_latitude']<=NEW_YORK_BOX_COORDS[1][0])]\n",
    "    \n",
    "    #for dropoff_longitude and dropoff_latitude\n",
    "    df = df[(df['dropoff_longitude']>=NEW_YORK_BOX_COORDS[0][1])\n",
    "            &(df['dropoff_longitude']<=NEW_YORK_BOX_COORDS[1][1])\n",
    "            &(df['dropoff_latitude']>=NEW_YORK_BOX_COORDS[0][0])\n",
    "            &(df['dropoff_latitude']<=NEW_YORK_BOX_COORDS[1][0])]\n",
    "    \n",
    "    #remove all trips with the same pickup_longitude and dropoff_longitude\n",
    "    #and the same pickup_latitude and dropoff_latitude\n",
    "    #which may result distance = 0 \n",
    "    #filter out these values\n",
    "    df = df[(df['pickup_longitude']!=df['dropoff_longitude'])\n",
    "           &(df['pickup_latitude']!=df['dropoff_latitude'])]\n",
    "    \n",
    "    #change the type of \"pickup_datetime\" into python datetime\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "    \n",
    "    #make sure all the data types are the desired ones\n",
    "    df = df.astype({'pickup_longitude':'float64','pickup_latitude':'float64',\n",
    "                    'dropoff_longitude':'float64','dropoff_latitude':'float64','tip_amount':'float64'})\n",
    "    \n",
    "    #since we need to match the amount of data from the taxi file with the one from the uber \n",
    "    #as we know that the cleaned uber data is about 195,000, \n",
    "    #we have total 78 months from 2009-01 to 2015-06, so 195,000/78 is about 2500 rows for each month\n",
    "    df = df.sample(2500)\n",
    "    \n",
    "    #return the cleaned taxi data frame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(parquet_urls):\n",
    "    \n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    for parquet_url in parquet_urls:\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        dataframe = get_and_clean_month(parquet_url)\n",
    "        add_distance_column(dataframe)\n",
    "        # maybe: if the file hasn't been saved, save it so you can\n",
    "        # avoid re-downloading it if you re-run the function\n",
    "        \n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    \n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "200776ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data():\n",
    "    \n",
    "    parquet_urls = get_all_yellow_taxi_urls(TAXI_URL)\n",
    "    taxi_data = get_and_clean_taxi_data(parquet_urls)\n",
    "    \n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "876bd645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"longitude\"]=shp.centroid.x\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/1136422984.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp[\"latitude\"]=shp.centroid.y\n"
     ]
    }
   ],
   "source": [
    "taxi_data = get_taxi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "529d9718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8581011</th>\n",
       "      <td>2015-01-21 18:07:02</td>\n",
       "      <td>-73.957012</td>\n",
       "      <td>40.780436</td>\n",
       "      <td>-73.978632</td>\n",
       "      <td>40.783961</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.862072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5878423</th>\n",
       "      <td>2015-01-15 12:21:49</td>\n",
       "      <td>-73.985156</td>\n",
       "      <td>40.748575</td>\n",
       "      <td>-73.984052</td>\n",
       "      <td>40.736824</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.309912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5136449</th>\n",
       "      <td>2015-01-13 18:39:35</td>\n",
       "      <td>-73.965146</td>\n",
       "      <td>40.756729</td>\n",
       "      <td>-73.945750</td>\n",
       "      <td>40.790011</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.045146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6301733</th>\n",
       "      <td>2015-01-16 10:38:57</td>\n",
       "      <td>-73.981532</td>\n",
       "      <td>40.773633</td>\n",
       "      <td>-73.978632</td>\n",
       "      <td>40.783961</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.174113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12202473</th>\n",
       "      <td>2015-01-31 02:20:49</td>\n",
       "      <td>-73.959905</td>\n",
       "      <td>40.710880</td>\n",
       "      <td>-73.990896</td>\n",
       "      <td>40.718938</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.761453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14486267</th>\n",
       "      <td>2009-12-22 09:57:08</td>\n",
       "      <td>-73.933309</td>\n",
       "      <td>40.795224</td>\n",
       "      <td>-73.991157</td>\n",
       "      <td>40.747186</td>\n",
       "      <td>4.85</td>\n",
       "      <td>7.229324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9379829</th>\n",
       "      <td>2009-12-29 14:55:00</td>\n",
       "      <td>-73.976635</td>\n",
       "      <td>40.764952</td>\n",
       "      <td>-73.987410</td>\n",
       "      <td>40.755588</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.381212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3235071</th>\n",
       "      <td>2009-12-31 09:03:06</td>\n",
       "      <td>-73.957543</td>\n",
       "      <td>40.765698</td>\n",
       "      <td>-74.006957</td>\n",
       "      <td>40.742594</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.891247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9700561</th>\n",
       "      <td>2009-12-02 07:54:47</td>\n",
       "      <td>-73.954295</td>\n",
       "      <td>40.770307</td>\n",
       "      <td>-73.978548</td>\n",
       "      <td>40.753523</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.766848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5787956</th>\n",
       "      <td>2009-12-29 15:33:00</td>\n",
       "      <td>-73.954345</td>\n",
       "      <td>40.786947</td>\n",
       "      <td>-73.959330</td>\n",
       "      <td>40.782953</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.611053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             pickup_datetime  pickup_longitude  pickup_latitude  \\\n",
       "8581011  2015-01-21 18:07:02        -73.957012        40.780436   \n",
       "5878423  2015-01-15 12:21:49        -73.985156        40.748575   \n",
       "5136449  2015-01-13 18:39:35        -73.965146        40.756729   \n",
       "6301733  2015-01-16 10:38:57        -73.981532        40.773633   \n",
       "12202473 2015-01-31 02:20:49        -73.959905        40.710880   \n",
       "...                      ...               ...              ...   \n",
       "14486267 2009-12-22 09:57:08        -73.933309        40.795224   \n",
       "9379829  2009-12-29 14:55:00        -73.976635        40.764952   \n",
       "3235071  2009-12-31 09:03:06        -73.957543        40.765698   \n",
       "9700561  2009-12-02 07:54:47        -73.954295        40.770307   \n",
       "5787956  2009-12-29 15:33:00        -73.954345        40.786947   \n",
       "\n",
       "          dropoff_longitude  dropoff_latitude  tip_amount  distance  \n",
       "8581011          -73.978632         40.783961        1.00  1.862072  \n",
       "5878423          -73.984052         40.736824        0.00  1.309912  \n",
       "5136449          -73.945750         40.790011        0.00  4.045146  \n",
       "6301733          -73.978632         40.783961        1.45  1.174113  \n",
       "12202473         -73.990896         40.718938        0.00  2.761453  \n",
       "...                     ...               ...         ...       ...  \n",
       "14486267         -73.991157         40.747186        4.85  7.229324  \n",
       "9379829          -73.987410         40.755588        0.00  1.381212  \n",
       "3235071          -74.006957         40.742594        2.00  4.891247  \n",
       "9700561          -73.978548         40.753523        0.00  2.766848  \n",
       "5787956          -73.959330         40.782953        0.00  0.611053  \n",
       "\n",
       "[195000 rows x 7 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "10ebd75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8581011</th>\n",
       "      <td>2015-01-21 18:07:02</td>\n",
       "      <td>-73.957012</td>\n",
       "      <td>40.780436</td>\n",
       "      <td>-73.978632</td>\n",
       "      <td>40.783961</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.862072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5878423</th>\n",
       "      <td>2015-01-15 12:21:49</td>\n",
       "      <td>-73.985156</td>\n",
       "      <td>40.748575</td>\n",
       "      <td>-73.984052</td>\n",
       "      <td>40.736824</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.309912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5136449</th>\n",
       "      <td>2015-01-13 18:39:35</td>\n",
       "      <td>-73.965146</td>\n",
       "      <td>40.756729</td>\n",
       "      <td>-73.945750</td>\n",
       "      <td>40.790011</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.045146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6301733</th>\n",
       "      <td>2015-01-16 10:38:57</td>\n",
       "      <td>-73.981532</td>\n",
       "      <td>40.773633</td>\n",
       "      <td>-73.978632</td>\n",
       "      <td>40.783961</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.174113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12202473</th>\n",
       "      <td>2015-01-31 02:20:49</td>\n",
       "      <td>-73.959905</td>\n",
       "      <td>40.710880</td>\n",
       "      <td>-73.990896</td>\n",
       "      <td>40.718938</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.761453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             pickup_datetime  pickup_longitude  pickup_latitude  \\\n",
       "8581011  2015-01-21 18:07:02        -73.957012        40.780436   \n",
       "5878423  2015-01-15 12:21:49        -73.985156        40.748575   \n",
       "5136449  2015-01-13 18:39:35        -73.965146        40.756729   \n",
       "6301733  2015-01-16 10:38:57        -73.981532        40.773633   \n",
       "12202473 2015-01-31 02:20:49        -73.959905        40.710880   \n",
       "\n",
       "          dropoff_longitude  dropoff_latitude  tip_amount  distance  \n",
       "8581011          -73.978632         40.783961        1.00  1.862072  \n",
       "5878423          -73.984052         40.736824        0.00  1.309912  \n",
       "5136449          -73.945750         40.790011        0.00  4.045146  \n",
       "6301733          -73.978632         40.783961        1.45  1.174113  \n",
       "12202473         -73.990896         40.718938        0.00  2.761453  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data():\n",
    "    \n",
    "    #load in the uber csv file as a data frame by pandas, and set the first column as index\n",
    "    df_uber = pd.read_csv(UBER_CSV, index_col=0)\n",
    "        \n",
    "    #removing the rows whose passenger count is less than or equal to 0, which is unpractical\n",
    "    df_uber = df_uber[(df_uber['fare_amount']>0)&(df_uber['passenger_count']>0)]\n",
    "    \n",
    "    #only keeping useful columns\n",
    "    df_uber = df_uber[['pickup_datetime','pickup_longitude','pickup_latitude',\n",
    "                       'dropoff_longitude','dropoff_latitude']]\n",
    "    \n",
    "    #removing all data outside of the New York Box range\n",
    "    \n",
    "    #for pickup_longitude and pickup_latitude\n",
    "    df_uber = df_uber[(df_uber['pickup_longitude']>=NEW_YORK_BOX_COORDS[0][1])\n",
    "                  &(df_uber['pickup_longitude']<=NEW_YORK_BOX_COORDS[1][1])\n",
    "                  &(df_uber['pickup_latitude']>=NEW_YORK_BOX_COORDS[0][0])\n",
    "                  &(df_uber['pickup_latitude']<=NEW_YORK_BOX_COORDS[1][0])]\n",
    "    \n",
    "    #for dropoff_longitude and dropoff_latitude\n",
    "    df_uber = df_uber[(df_uber['dropoff_longitude']>=NEW_YORK_BOX_COORDS[0][1])\n",
    "                  &(df_uber['dropoff_longitude']<=NEW_YORK_BOX_COORDS[1][1])\n",
    "                  &(df_uber['dropoff_latitude']>=NEW_YORK_BOX_COORDS[0][0])\n",
    "                  &(df_uber['dropoff_latitude']<=NEW_YORK_BOX_COORDS[1][0])]\n",
    "    \n",
    "    #filter out distance = 0 \n",
    "    df_uber = df_uber[(df_uber['pickup_longitude']!=df_uber['dropoff_longitude'])\n",
    "                      &(df_uber['pickup_latitude']!=df_uber['dropoff_latitude'])]\n",
    "    \n",
    "    #change the type of \"pickup_datetime\" into python datetime\n",
    "    df_uber['pickup_datetime'] = pd.to_datetime(df_uber['pickup_datetime'])\n",
    "    \n",
    "    #change all the data types into desired ones\n",
    "    df_uber = df_uber.astype({'pickup_longitude':'float64','pickup_latitude':'float64',\n",
    "                              'dropoff_longitude':'float64','dropoff_latitude':'float64'})\n",
    "    \n",
    "    #return the cleaned uber data frame\n",
    "    return df_uber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    uber_dataframe = load_and_clean_uber_data()\n",
    "    add_distance_column(uber_dataframe)\n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9c2bd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4e533273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24238194</th>\n",
       "      <td>2015-05-07 19:52:06+00:00</td>\n",
       "      <td>-73.999817</td>\n",
       "      <td>40.738354</td>\n",
       "      <td>-73.999512</td>\n",
       "      <td>40.723217</td>\n",
       "      <td>1.683323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27835199</th>\n",
       "      <td>2009-07-17 20:04:56+00:00</td>\n",
       "      <td>-73.994355</td>\n",
       "      <td>40.728225</td>\n",
       "      <td>-73.994710</td>\n",
       "      <td>40.750325</td>\n",
       "      <td>2.457590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44984355</th>\n",
       "      <td>2009-08-24 21:45:00+00:00</td>\n",
       "      <td>-74.005043</td>\n",
       "      <td>40.740770</td>\n",
       "      <td>-73.962565</td>\n",
       "      <td>40.772647</td>\n",
       "      <td>5.036377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25894730</th>\n",
       "      <td>2009-06-26 08:22:21+00:00</td>\n",
       "      <td>-73.976124</td>\n",
       "      <td>40.790844</td>\n",
       "      <td>-73.965316</td>\n",
       "      <td>40.803349</td>\n",
       "      <td>1.661683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17610152</th>\n",
       "      <td>2014-08-28 17:47:00+00:00</td>\n",
       "      <td>-73.925023</td>\n",
       "      <td>40.744085</td>\n",
       "      <td>-73.973082</td>\n",
       "      <td>40.761247</td>\n",
       "      <td>4.475450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42598914</th>\n",
       "      <td>2012-10-28 10:49:00+00:00</td>\n",
       "      <td>-73.987042</td>\n",
       "      <td>40.739367</td>\n",
       "      <td>-73.986525</td>\n",
       "      <td>40.740297</td>\n",
       "      <td>0.112210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16382965</th>\n",
       "      <td>2014-03-14 01:09:00+00:00</td>\n",
       "      <td>-73.984722</td>\n",
       "      <td>40.736837</td>\n",
       "      <td>-74.006672</td>\n",
       "      <td>40.739620</td>\n",
       "      <td>1.875050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27804658</th>\n",
       "      <td>2009-06-29 00:42:00+00:00</td>\n",
       "      <td>-73.986017</td>\n",
       "      <td>40.756487</td>\n",
       "      <td>-73.858957</td>\n",
       "      <td>40.692588</td>\n",
       "      <td>12.850319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20259894</th>\n",
       "      <td>2015-05-20 14:56:25+00:00</td>\n",
       "      <td>-73.997124</td>\n",
       "      <td>40.725452</td>\n",
       "      <td>-73.983215</td>\n",
       "      <td>40.695415</td>\n",
       "      <td>3.539715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11951496</th>\n",
       "      <td>2010-05-15 04:08:00+00:00</td>\n",
       "      <td>-73.984395</td>\n",
       "      <td>40.720077</td>\n",
       "      <td>-73.985508</td>\n",
       "      <td>40.768793</td>\n",
       "      <td>5.417783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192738 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   pickup_datetime  pickup_longitude  pickup_latitude  \\\n",
       "24238194 2015-05-07 19:52:06+00:00        -73.999817        40.738354   \n",
       "27835199 2009-07-17 20:04:56+00:00        -73.994355        40.728225   \n",
       "44984355 2009-08-24 21:45:00+00:00        -74.005043        40.740770   \n",
       "25894730 2009-06-26 08:22:21+00:00        -73.976124        40.790844   \n",
       "17610152 2014-08-28 17:47:00+00:00        -73.925023        40.744085   \n",
       "...                            ...               ...              ...   \n",
       "42598914 2012-10-28 10:49:00+00:00        -73.987042        40.739367   \n",
       "16382965 2014-03-14 01:09:00+00:00        -73.984722        40.736837   \n",
       "27804658 2009-06-29 00:42:00+00:00        -73.986017        40.756487   \n",
       "20259894 2015-05-20 14:56:25+00:00        -73.997124        40.725452   \n",
       "11951496 2010-05-15 04:08:00+00:00        -73.984395        40.720077   \n",
       "\n",
       "          dropoff_longitude  dropoff_latitude   distance  \n",
       "24238194         -73.999512         40.723217   1.683323  \n",
       "27835199         -73.994710         40.750325   2.457590  \n",
       "44984355         -73.962565         40.772647   5.036377  \n",
       "25894730         -73.965316         40.803349   1.661683  \n",
       "17610152         -73.973082         40.761247   4.475450  \n",
       "...                     ...               ...        ...  \n",
       "42598914         -73.986525         40.740297   0.112210  \n",
       "16382965         -74.006672         40.739620   1.875050  \n",
       "27804658         -73.858957         40.692588  12.850319  \n",
       "20259894         -73.983215         40.695415   3.539715  \n",
       "11951496         -73.985508         40.768793   5.417783  \n",
       "\n",
       "[192738 rows x 6 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "339997e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24238194</th>\n",
       "      <td>2015-05-07 19:52:06+00:00</td>\n",
       "      <td>-73.999817</td>\n",
       "      <td>40.738354</td>\n",
       "      <td>-73.999512</td>\n",
       "      <td>40.723217</td>\n",
       "      <td>1.683323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27835199</th>\n",
       "      <td>2009-07-17 20:04:56+00:00</td>\n",
       "      <td>-73.994355</td>\n",
       "      <td>40.728225</td>\n",
       "      <td>-73.994710</td>\n",
       "      <td>40.750325</td>\n",
       "      <td>2.457590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44984355</th>\n",
       "      <td>2009-08-24 21:45:00+00:00</td>\n",
       "      <td>-74.005043</td>\n",
       "      <td>40.740770</td>\n",
       "      <td>-73.962565</td>\n",
       "      <td>40.772647</td>\n",
       "      <td>5.036377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25894730</th>\n",
       "      <td>2009-06-26 08:22:21+00:00</td>\n",
       "      <td>-73.976124</td>\n",
       "      <td>40.790844</td>\n",
       "      <td>-73.965316</td>\n",
       "      <td>40.803349</td>\n",
       "      <td>1.661683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17610152</th>\n",
       "      <td>2014-08-28 17:47:00+00:00</td>\n",
       "      <td>-73.925023</td>\n",
       "      <td>40.744085</td>\n",
       "      <td>-73.973082</td>\n",
       "      <td>40.761247</td>\n",
       "      <td>4.475450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   pickup_datetime  pickup_longitude  pickup_latitude  \\\n",
       "24238194 2015-05-07 19:52:06+00:00        -73.999817        40.738354   \n",
       "27835199 2009-07-17 20:04:56+00:00        -73.994355        40.728225   \n",
       "44984355 2009-08-24 21:45:00+00:00        -74.005043        40.740770   \n",
       "25894730 2009-06-26 08:22:21+00:00        -73.976124        40.790844   \n",
       "17610152 2014-08-28 17:47:00+00:00        -73.925023        40.744085   \n",
       "\n",
       "          dropoff_longitude  dropoff_latitude  distance  \n",
       "24238194         -73.999512         40.723217  1.683323  \n",
       "27835199         -73.994710         40.750325  2.457590  \n",
       "44984355         -73.962565         40.772647  5.036377  \n",
       "25894730         -73.965316         40.803349  1.661683  \n",
       "17610152         -73.973082         40.761247  4.475450  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0ec5370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs():\n",
    "    \n",
    "    #initiate with an empty list \n",
    "    weather_csvs = []\n",
    "    \n",
    "    #iterate the year from 2009 to 2015\n",
    "    for year in range(2009, 2016, 1):\n",
    "        weather_csv = 'data/' + str(year) + '_weather.csv'\n",
    "        weather_csvs.append(weather_csv)\n",
    "    \n",
    "    #return a list with all weathers files names from 2009 to 2015\n",
    "    return weather_csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8d8d210e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/2009_weather.csv',\n",
       " 'data/2010_weather.csv',\n",
       " 'data/2011_weather.csv',\n",
       " 'data/2012_weather.csv',\n",
       " 'data/2013_weather.csv',\n",
       " 'data/2014_weather.csv',\n",
       " 'data/2015_weather.csv']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_weather_csvs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_weather_data_hourly(csv_file):\n",
    "    \n",
    "    #load a certain weather csv file into data frame\n",
    "    df_weather = pd.read_csv(csv_file)\n",
    "    \n",
    "    #transform the data type of column 'Date' into a datetime format\n",
    "    df_weather['DATE'] = pd.to_datetime(df_weather['DATE'])\n",
    "    \n",
    "    #split hour and minute of a datetime and create corresponding columns respectively\n",
    "    df_weather['timestamp_hour'] = df_weather['DATE'].apply(lambda x:str(x.hour).zfill(2))\n",
    "    df_weather['timestamp_minute'] = df_weather['DATE'].apply(lambda x:str(x.minute).zfill(2))\n",
    "\n",
    "    #as I manually observe that the daily data is always collected at the last minute of that day\n",
    "    #I create a new data frame by filtering the datetime as the last minute of a day\n",
    "    df_weather_day_end = df_weather[(df_weather['timestamp_hour'] == '23')\n",
    "                                & (df_weather['timestamp_minute'] == '59')]\n",
    "    \n",
    "    #use the index to drop all daily collected weather data from the total weather data\n",
    "    #after that, we can get the pure hourly weather data \n",
    "    df_weather_hourly = df_weather.drop(df_weather_day_end.index)\n",
    "    \n",
    "    #only keep the useful columns\n",
    "    df_weather_hourly = df_weather_hourly[['DATE','HourlyPrecipitation','HourlyWindSpeed']]\n",
    "    \n",
    "    # https://stackoverflow.com/questions/58807577/pandas-dataframe-extracting-float-values-from-string-in-a-column\n",
    "    #since I found that there are some data with units, strip all the units and only keep the numeric values\n",
    "    df_weather_hourly['HourlyPrecipitation'] = pd.to_numeric(\n",
    "        df_weather_hourly['HourlyPrecipitation'].str.extract(r'(\\d+\\.?\\d*)', expand=False), errors='coerce')\n",
    "    \n",
    "    #As the documentation mentions that the blank/null values indicate that\n",
    "    #no precipitation was observed/reported for the hour ending at that time\n",
    "    #also, the value of 'T' means trace amount of precipitation, \n",
    "    #therefore, we can directly fill all these values as 0\n",
    "    df_weather_hourly['HourlyPrecipitation'] = df_weather_hourly['HourlyPrecipitation'].fillna(0)\n",
    "    \n",
    "    #since there are some rows whose hourly wind speed is null\n",
    "    #as this is a very small amount, we can drop these values, which will not affect the whole trend\n",
    "    #by the law of large numbers\n",
    "    df_weather_hourly = df_weather_hourly.dropna(subset='HourlyWindSpeed')\n",
    "    \n",
    "    #reset the index\n",
    "    df_weather_hourly.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    #need to check if this is needed\n",
    "    #rename the essential columns\n",
    "    df_weather_hourly.rename(columns={'DATE':'date','HourlyPrecipitation':'hourly_precipitation',\n",
    "                                      'HourlyWindSpeed':'hourly_wind_speed'}, inplace=True)\n",
    "    \n",
    "    #make sure all the data types are the desired ones\n",
    "    df_weather_hourly = df_weather_hourly.astype({'hourly_precipitation':'float64',\n",
    "                                                  'hourly_wind_speed':'float64'})\n",
    "    \n",
    "    #need to check if this is needed\n",
    "    #make sure the 'Date' columns follow datetime format\n",
    "    df_weather_hourly['date'] = pd.to_datetime(df_weather_hourly['date'])\n",
    "    \n",
    "    #return the cleaned hourly weather data\n",
    "    return df_weather_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_weather_data_daily(csv_file):\n",
    "    \n",
    "    #load a certain weather csv file into data frame\n",
    "    df_weather = pd.read_csv(csv_file)\n",
    "    \n",
    "    #transform the data type of column 'Date' into a datetime format\n",
    "    df_weather['DATE'] = pd.to_datetime(df_weather['DATE'])\n",
    "    \n",
    "    #split hour and minute of a datetime and create corresponding columns respectively\n",
    "    df_weather['timestamp_hour'] = df_weather['DATE'].apply(lambda x:str(x.hour).zfill(2))\n",
    "    df_weather['timestamp_minute'] = df_weather['DATE'].apply(lambda x:str(x.minute).zfill(2))\n",
    "\n",
    "    #as I manually observe that the daily data is always collected at the last minute of that day\n",
    "    #I create a new data frame by filtering the datetime as the last minute of a day\n",
    "    df_weather_day_end = df_weather[(df_weather['timestamp_hour'] == '23')\n",
    "                                & (df_weather['timestamp_minute'] == '59')]\n",
    "    \n",
    "    #use the index to drop all daily collected weather data from the total weather data\n",
    "    #after that, we can get the pure hourly weather data \n",
    "    df_weather_hourly = df_weather.drop(df_weather_day_end.index)\n",
    "    \n",
    "    #only keep the useful columns\n",
    "    df_weather_hourly = df_weather_hourly[['DATE','HourlyPrecipitation','HourlyWindSpeed']]\n",
    "    \n",
    "    # https://stackoverflow.com/questions/58807577/pandas-dataframe-extracting-float-values-from-string-in-a-column\n",
    "    #found some data with units, clear all the units and only keep the numeric values\n",
    "    df_weather_hourly['HourlyPrecipitation'] = pd.to_numeric(\n",
    "        df_weather_hourly['HourlyPrecipitation'].str.extract(r'(\\d+\\.?\\d*)', expand=False), errors='coerce')\n",
    "    \n",
    "    #As the documentation mentions that the blank/null values indicate that\n",
    "    #no precipitation was observed/reported for the hour ending at that time\n",
    "    #also, the value of 'T' means trace amount of precipitation, \n",
    "    #therefore, we can directly fill all these values as 0\n",
    "    df_weather_hourly['HourlyPrecipitation'] = df_weather_hourly['HourlyPrecipitation'].fillna(0)\n",
    "    \n",
    "    #since there are some rows whose hourly wind speed is null\n",
    "    #as this is a very small amount, we can drop these values, which will not affect the whole trend\n",
    "    #by the law of large numbers\n",
    "    df_weather_hourly = df_weather_hourly.dropna(subset='HourlyWindSpeed')\n",
    "    \n",
    "    #since we can get mostly the hourly data from all weather files\n",
    "    #however, there are too many missing values in a daily frequency, especially from 2009 to 2012\n",
    "    #using hourly data to populate the daily data \n",
    "    #also, by observing the daily precipitation and daily average wind speed columns from 2013 to 2015\n",
    "    #the way to calculate the daily precipitation is to take summation\n",
    "    #the way to calculate the daily average wind speed is to take average of samples in a 24-hours interval\n",
    "    df_weather_daily = df_weather_hourly.groupby(\n",
    "        df_weather_hourly['DATE'].dt.date).agg({'HourlyPrecipitation': 'sum', 'HourlyWindSpeed': 'mean'})\n",
    "    \n",
    "    #since the index for daily df is the date, create another date column DATE with index values\n",
    "    df_weather_daily['DATE'] = df_weather_daily.index\n",
    "    \n",
    "    #make sure the DATE as the datetime format\n",
    "    df_weather_daily['DATE'] = pd.to_datetime(df_weather_daily['DATE'])\n",
    "    \n",
    "    #reset the default index as row number\n",
    "    df_weather_daily.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    #rename the essential columns\n",
    "    df_weather_daily.rename(columns={'DATE':'date',\n",
    "                                     'HourlyPrecipitation':'daily_precipitation',\n",
    "                                     'HourlyWindSpeed':'daily_average_wind_speed'}, inplace=True)\n",
    "    \n",
    "    #make sure all the data types are the desired ones\n",
    "    df_weather_daily = df_weather_daily.astype({'daily_precipitation':'float64',\n",
    "                                                'daily_average_wind_speed':'float64'})\n",
    "    \n",
    "    #rearrange the order of all columns\n",
    "    df_weather_daily = df_weather_daily.reindex(columns=['date','daily_precipitation','daily_average_wind_speed'])\n",
    "        \n",
    "    #return the cleaned version of daily weather data\n",
    "    return df_weather_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "3654db7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_sunrise_sunset_data(csv_file):\n",
    "    \n",
    "    #load a certain weather csv file into data frame\n",
    "    df_weather = pd.read_csv(csv_file)\n",
    "    \n",
    "    #transform the data type of column 'Date' into a datetime format\n",
    "    df_weather['DATE'] = pd.to_datetime(df_weather['DATE'])\n",
    "    \n",
    "    #split hour and minute of a datetime and create corresponding columns respectively\n",
    "    df_weather['timestamp_hour'] = df_weather['DATE'].apply(lambda x:str(x.hour).zfill(2))\n",
    "    df_weather['timestamp_minute'] = df_weather['DATE'].apply(lambda x:str(x.minute).zfill(2))\n",
    "\n",
    "    #as I manually observe that the daily data is always collected at the last minute of that day\n",
    "    #create a new data frame with only the daily data\n",
    "    df_weather_sun = df_weather[(df_weather['timestamp_hour'] == '23')\n",
    "                                & (df_weather['timestamp_minute'] == '59')]\n",
    "    \n",
    "    #only keep the important columns\n",
    "    df_weather_sun = df_weather_sun[['DATE','Sunrise','Sunset']]\n",
    "    \n",
    "    #since we can't use any other columns to populate the null values\n",
    "    #directly drop the null values\n",
    "    df_weather_sun = df_weather_sun.dropna(subset=['Sunrise','Sunset'])\n",
    "    \n",
    "    #rename the essential columns\n",
    "    df_weather_sun.rename(columns={'DATE':'date','Sunrise':'sunrise',\n",
    "                                   'Sunset':'sunset'}, inplace=True)\n",
    "    \n",
    "    #rearrange the index, counting from zero and increment\n",
    "    df_weather_sun.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    #make sure the DATE as the datetime format\n",
    "    df_weather_sun['date'] = pd.to_datetime(df_weather_sun['date'])\n",
    "    \n",
    "    return df_weather_sun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    \n",
    "    weather_csv_files = get_all_weather_csvs()\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "    sun_dataframes = []\n",
    "        \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = get_and_clean_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = get_and_clean_weather_data_daily(csv_file)\n",
    "        sun_dataframe = get_and_clean_sunrise_sunset_data(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        sun_dataframes.append(sun_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every year\n",
    "    # create one dataframe with only sunrise and sunset data\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    sun_data = pd.concat(sun_dataframes)\n",
    "    \n",
    "    #make sure all index are counting the row number from 0 and gradually incrementing\n",
    "    hourly_data.reset_index(inplace=True, drop=True)\n",
    "    daily_data.reset_index(inplace=True, drop=True)\n",
    "    sun_data.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return hourly_data, daily_data, sun_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "0f511be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/3617500815.py:4: DtypeWarning: Columns (9,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_weather = pd.read_csv(csv_file)\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/3538953825.py:4: DtypeWarning: Columns (9,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_weather = pd.read_csv(csv_file)\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/492313200.py:4: DtypeWarning: Columns (9,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_weather = pd.read_csv(csv_file)\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/3617500815.py:4: DtypeWarning: Columns (8,9,10,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_weather = pd.read_csv(csv_file)\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/3538953825.py:4: DtypeWarning: Columns (8,9,10,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_weather = pd.read_csv(csv_file)\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/492313200.py:4: DtypeWarning: Columns (8,9,10,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_weather = pd.read_csv(csv_file)\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/3617500815.py:4: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_weather = pd.read_csv(csv_file)\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/3538953825.py:4: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_weather = pd.read_csv(csv_file)\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/492313200.py:4: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_weather = pd.read_csv(csv_file)\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/3617500815.py:4: DtypeWarning: Columns (7,8,9,10,17,18,42,65) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_weather = pd.read_csv(csv_file)\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/3538953825.py:4: DtypeWarning: Columns (7,8,9,10,17,18,42,65) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_weather = pd.read_csv(csv_file)\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/492313200.py:4: DtypeWarning: Columns (7,8,9,10,17,18,42,65) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_weather = pd.read_csv(csv_file)\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/3617500815.py:4: DtypeWarning: Columns (17,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_weather = pd.read_csv(csv_file)\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/3538953825.py:4: DtypeWarning: Columns (17,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_weather = pd.read_csv(csv_file)\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/492313200.py:4: DtypeWarning: Columns (17,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_weather = pd.read_csv(csv_file)\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/3617500815.py:4: DtypeWarning: Columns (8,9,17,18,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_weather = pd.read_csv(csv_file)\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/3538953825.py:4: DtypeWarning: Columns (8,9,17,18,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_weather = pd.read_csv(csv_file)\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/492313200.py:4: DtypeWarning: Columns (8,9,17,18,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_weather = pd.read_csv(csv_file)\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/3617500815.py:4: DtypeWarning: Columns (10,41,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_weather = pd.read_csv(csv_file)\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/3538953825.py:4: DtypeWarning: Columns (10,41,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_weather = pd.read_csv(csv_file)\n",
      "/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_884/492313200.py:4: DtypeWarning: Columns (10,41,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_weather = pd.read_csv(csv_file)\n"
     ]
    }
   ],
   "source": [
    "hourly_data, daily_data, sun_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "48216557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hourly_precipitation</th>\n",
       "      <th>hourly_wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-01 00:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-01 01:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-01 02:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-01 03:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-01 04:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date  hourly_precipitation  hourly_wind_speed\n",
       "0 2009-01-01 00:51:00                   0.0               18.0\n",
       "1 2009-01-01 01:51:00                   0.0               18.0\n",
       "2 2009-01-01 02:51:00                   0.0               18.0\n",
       "3 2009-01-01 03:51:00                   0.0                8.0\n",
       "4 2009-01-01 04:51:00                   0.0               11.0"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "a6847c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hourly_precipitation</th>\n",
       "      <th>hourly_wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-01 00:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-01 01:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-01 02:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-01 03:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-01 04:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73702</th>\n",
       "      <td>2015-12-31 18:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73703</th>\n",
       "      <td>2015-12-31 19:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73704</th>\n",
       "      <td>2015-12-31 20:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73705</th>\n",
       "      <td>2015-12-31 22:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73706</th>\n",
       "      <td>2015-12-31 23:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73707 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     date  hourly_precipitation  hourly_wind_speed\n",
       "0     2009-01-01 00:51:00                   0.0               18.0\n",
       "1     2009-01-01 01:51:00                   0.0               18.0\n",
       "2     2009-01-01 02:51:00                   0.0               18.0\n",
       "3     2009-01-01 03:51:00                   0.0                8.0\n",
       "4     2009-01-01 04:51:00                   0.0               11.0\n",
       "...                   ...                   ...                ...\n",
       "73702 2015-12-31 18:51:00                   0.0                3.0\n",
       "73703 2015-12-31 19:51:00                   0.0                6.0\n",
       "73704 2015-12-31 20:51:00                   0.0               10.0\n",
       "73705 2015-12-31 22:51:00                   0.0                7.0\n",
       "73706 2015-12-31 23:51:00                   0.0                5.0\n",
       "\n",
       "[73707 rows x 3 columns]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>daily_precipitation</th>\n",
       "      <th>daily_average_wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.806452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.370370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.925926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  daily_precipitation  daily_average_wind_speed\n",
       "0 2009-01-01                  0.0                 11.041667\n",
       "1 2009-01-02                  0.0                  6.806452\n",
       "2 2009-01-03                  0.0                  9.875000\n",
       "3 2009-01-04                  0.0                  7.370370\n",
       "4 2009-01-05                  0.0                  6.925926"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "daa7afce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>daily_precipitation</th>\n",
       "      <th>daily_average_wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.806452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.370370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.925926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536</th>\n",
       "      <td>2015-12-27</td>\n",
       "      <td>0.17</td>\n",
       "      <td>4.911111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537</th>\n",
       "      <td>2015-12-28</td>\n",
       "      <td>0.03</td>\n",
       "      <td>8.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2538</th>\n",
       "      <td>2015-12-29</td>\n",
       "      <td>0.93</td>\n",
       "      <td>7.787234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2539</th>\n",
       "      <td>2015-12-30</td>\n",
       "      <td>0.29</td>\n",
       "      <td>4.184211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2540</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.741935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2541 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  daily_precipitation  daily_average_wind_speed\n",
       "0    2009-01-01                 0.00                 11.041667\n",
       "1    2009-01-02                 0.00                  6.806452\n",
       "2    2009-01-03                 0.00                  9.875000\n",
       "3    2009-01-04                 0.00                  7.370370\n",
       "4    2009-01-05                 0.00                  6.925926\n",
       "...         ...                  ...                       ...\n",
       "2536 2015-12-27                 0.17                  4.911111\n",
       "2537 2015-12-28                 0.03                  8.208333\n",
       "2538 2015-12-29                 0.93                  7.787234\n",
       "2539 2015-12-30                 0.29                  4.184211\n",
       "2540 2015-12-31                 0.08                  4.741935\n",
       "\n",
       "[2541 rows x 3 columns]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "9ac6dd5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>sunrise</th>\n",
       "      <th>sunset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-02 23:59:00</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1640.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-06 23:59:00</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1644.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-07 23:59:00</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-10 23:59:00</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1648.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-11 23:59:00</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1649.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date  sunrise  sunset\n",
       "0 2009-01-02 23:59:00    720.0  1640.0\n",
       "1 2009-01-06 23:59:00    720.0  1644.0\n",
       "2 2009-01-07 23:59:00    720.0  1645.0\n",
       "3 2009-01-10 23:59:00    720.0  1648.0\n",
       "4 2009-01-11 23:59:00    720.0  1649.0"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sun_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "83b9383b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>sunrise</th>\n",
       "      <th>sunset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-02 23:59:00</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1640.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-06 23:59:00</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1644.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-07 23:59:00</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-10 23:59:00</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1648.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-11 23:59:00</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1649.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1821</th>\n",
       "      <td>2015-12-27 23:59:00</td>\n",
       "      <td>719.0</td>\n",
       "      <td>1635.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>2015-12-28 23:59:00</td>\n",
       "      <td>719.0</td>\n",
       "      <td>1636.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>2015-12-29 23:59:00</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1636.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824</th>\n",
       "      <td>2015-12-30 23:59:00</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1637.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>2015-12-31 23:59:00</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1638.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1826 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    date  sunrise  sunset\n",
       "0    2009-01-02 23:59:00    720.0  1640.0\n",
       "1    2009-01-06 23:59:00    720.0  1644.0\n",
       "2    2009-01-07 23:59:00    720.0  1645.0\n",
       "3    2009-01-10 23:59:00    720.0  1648.0\n",
       "4    2009-01-11 23:59:00    720.0  1649.0\n",
       "...                  ...      ...     ...\n",
       "1821 2015-12-27 23:59:00    719.0  1635.0\n",
       "1822 2015-12-28 23:59:00    719.0  1636.0\n",
       "1823 2015-12-29 23:59:00    720.0  1636.0\n",
       "1824 2015-12-30 23:59:00    720.0  1637.0\n",
       "1825 2015-12-31 23:59:00    720.0  1638.0\n",
       "\n",
       "[1826 rows x 3 columns]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sun_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    date DATE,\n",
    "    hourly_precipitation FLOAT,\n",
    "    hourly_wind_speed FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    date DATE,\n",
    "    daily_precipitation FLOAT,\n",
    "    daily_average_wind_speed FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# SUN_WEATHER_SCHEMA = \"\"\"\n",
    "#     CREATE TABLE IF NOT EXISTS sun_weather\n",
    "#     (\n",
    "#         id INTEGER PRIMARY KEY,\n",
    "#         Date DATETIME,\n",
    "#         Sunset FLOAT64,\n",
    "#         Sunrise FLOAT64\n",
    "#     );\n",
    "# \"\"\"ed\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    pickup_datetime DATE,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    tip_amount FLOAT,\n",
    "    distance FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    pickup_datetime DATE,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    distance FLOAT\n",
    "    );\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(HOURLY_WEATHER_SCHEMA)\n",
    "    connection.execute(DAILY_WEATHER_SCHEMA)\n",
    "    connection.execute(TAXI_TRIPS_SCHEMA)\n",
    "    connection.execute(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "22fbfebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    \n",
    "    for table_name, dataframe in map_table_name_to_dataframe.items():\n",
    "        dataframe.to_sql(table_name, con=engine, if_exists='append', index=False)\n",
    "    \n",
    "    return 'Mission Completed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "74004f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mission Completed'"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    with open(outfile, 'w') as f:\n",
    "        f.write(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7ac515",
   "metadata": {},
   "source": [
    "#### For 01-2009 through 06-2015, show the popularity of Yellow Taxi rides for each hour of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"taxi_each_hour_popularity.sql\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "SELECT \n",
    "    strftime('%H', pickup_datetime) AS hour,\n",
    "    COUNT(id) AS popularity\n",
    "FROM \n",
    "    taxi_trips\n",
    "WHERE\n",
    "    pickup_datetime BETWEEN '2009-01-01' AND '2015-06-30'\n",
    "GROUP BY\n",
    "    hour\n",
    "ORDER BY\n",
    "    popularity DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('19', 12383),\n",
       " ('18', 11690),\n",
       " ('20', 11432),\n",
       " ('21', 11211),\n",
       " ('22', 11054),\n",
       " ('23', 9756),\n",
       " ('14', 9747),\n",
       " ('17', 9732),\n",
       " ('12', 9653),\n",
       " ('13', 9412),\n",
       " ('15', 9240),\n",
       " ('09', 9011),\n",
       " ('11', 8818),\n",
       " ('08', 8763),\n",
       " ('10', 8725),\n",
       " ('16', 8096),\n",
       " ('00', 7846),\n",
       " ('07', 6925),\n",
       " ('01', 5868),\n",
       " ('02', 4294),\n",
       " ('06', 3972),\n",
       " ('03', 3089),\n",
       " ('04', 2287),\n",
       " ('05', 1913)]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(QUERY_1).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "3588ecc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(engine.execute(QUERY_1).fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de050147",
   "metadata": {},
   "source": [
    "### Query 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920d6278",
   "metadata": {},
   "source": [
    "#### For the same time frame, show the popularity of Uber rides for each day of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "03006125",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_2_FILENAME = \"uber_each_day_popularity.sql\"\n",
    "\n",
    "QUERY_2 = \"\"\"\n",
    "SELECT \n",
    "    strftime('%w', pickup_datetime) AS day,\n",
    "    COUNT(id) as popularity\n",
    "FROM \n",
    "    uber_trips\n",
    "WHERE\n",
    "    pickup_datetime BETWEEN '2009-01-01' AND '2015-06-30'\n",
    "GROUP BY\n",
    "    day\n",
    "ORDER BY\n",
    "    popularity DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "2e870540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5', 29732),\n",
       " ('6', 29198),\n",
       " ('4', 28949),\n",
       " ('3', 27962),\n",
       " ('2', 27071),\n",
       " ('0', 25493),\n",
       " ('1', 24269)]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(QUERY_2).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "e2528db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_2, QUERY_2_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b663a774",
   "metadata": {},
   "source": [
    "### Query 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f1a570",
   "metadata": {},
   "source": [
    "#### What is the 95% percentile of distance traveled for all hired trips during July 2013?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "2ddf6f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_3_FILENAME = \"hired_trips_95%_percentile_July.sql\"\n",
    "\n",
    "QUERY_3 = \"\"\"\n",
    "WITH hired_trips AS (\n",
    "    SELECT pickup_datetime, distance \n",
    "    FROM taxi_trips\n",
    "    WHERE pickup_datetime BETWEEN '2013-07-01' AND '2013-07-31'\n",
    "    UNION ALL\n",
    "    SELECT pickup_datetime, distance\n",
    "    FROM uber_trips\n",
    "    WHERE pickup_datetime BETWEEN '2013-07-01' AND '2013-07-31'\n",
    "    )\n",
    "SELECT \n",
    "    distance\n",
    "FROM \n",
    "    hired_trips\n",
    "WHERE \n",
    "    pickup_datetime BETWEEN '2013-07-01' AND '2013-07-31'\n",
    "ORDER BY \n",
    "    distance ASC\n",
    "LIMIT 1\n",
    "OFFSET (\n",
    "    SELECT COUNT(*) \n",
    "    FROM hired_trips\n",
    "    WHERE pickup_datetime BETWEEN '2013-07-01' AND '2013-07-31'\n",
    "    ) * 95 / 100 - 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "b67126fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10.41196195829923,)]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(QUERY_3).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "e17a7099",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_3, QUERY_3_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff6c0b6",
   "metadata": {},
   "source": [
    "### Query 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98df5f82",
   "metadata": {},
   "source": [
    "#### What were the top 10 days with the highest number of hired rides for 2009, and what was the average distance for each day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "febb2cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_4_FILENAME = \"top_10_days_highest_with_avg_distance.sql\"\n",
    "\n",
    "QUERY_4 = \"\"\"\n",
    "WITH hired_trips AS (\n",
    "    SELECT pickup_datetime, distance\n",
    "    FROM taxi_trips\n",
    "    WHERE pickup_datetime BETWEEN '2009-01-01' AND '2009-12-31'\n",
    "    UNION ALL\n",
    "    SELECT pickup_datetime, distance\n",
    "    FROM uber_trips\n",
    "    WHERE pickup_datetime BETWEEN '2009-01-01' AND '2009-12-31'\n",
    "    )\n",
    "SELECT \n",
    "    DATE(pickup_datetime) as date, \n",
    "    COUNT(*) as total_number, \n",
    "    AVG(distance) as avg_distance\n",
    "FROM hired_trips\n",
    "GROUP BY date\n",
    "ORDER BY total_number DESC\n",
    "LIMIT 10\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "07b822d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2009-12-11', 221, 3.016592875793254),\n",
       " ('2009-02-20', 218, 3.1089744402912753),\n",
       " ('2009-04-03', 216, 2.5835324801263675),\n",
       " ('2009-10-23', 214, 2.883515238241926),\n",
       " ('2009-05-16', 213, 2.887044715760733),\n",
       " ('2009-02-12', 212, 2.9078997199046563),\n",
       " ('2009-06-05', 208, 2.928685023638607),\n",
       " ('2009-09-18', 207, 3.2136122283866233),\n",
       " ('2009-08-14', 207, 3.403669204801021),\n",
       " ('2009-01-24', 204, 2.5279740822104353)]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(QUERY_4).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "55860980",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_4, QUERY_4_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db949df5",
   "metadata": {},
   "source": [
    "### Query 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efec536a",
   "metadata": {},
   "source": [
    "#### Which 10 days in 2014 were the windiest on average, and how many hired trips were made on those days?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "6aa27a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_5_FILENAME = \"top_10_windiest_days_with_hired_trips.sql\"\n",
    "\n",
    "QUERY_5 = \"\"\"\n",
    "SELECT \n",
    "    DATE(daily_weather.date) as date,\n",
    "    daily_weather.daily_average_wind_speed as daily_avg,\n",
    "    COUNT(*)\n",
    "FROM (\n",
    "    SELECT pickup_datetime \n",
    "    FROM taxi_trips\n",
    "    WHERE pickup_datetime BETWEEN '2014-01-01' AND '2014-12-31'\n",
    "    UNION ALL\n",
    "    SELECT pickup_datetime\n",
    "    FROM uber_trips\n",
    "    WHERE pickup_datetime BETWEEN '2014-01-01' AND '2014-12-31'\n",
    "    ) AS hired_trips\n",
    "INNER JOIN daily_weather\n",
    "ON \n",
    "    DATE(hired_trips.pickup_datetime) = DATE(daily_weather.date)\n",
    "WHERE \n",
    "    date BETWEEN '2014-01-01' AND '2014-12-31'\n",
    "GROUP BY \n",
    "    date\n",
    "ORDER BY \n",
    "    daily_avg DESC\n",
    "LIMIT 10 \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "1fc57efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2014-03-13', 14.0, 192),\n",
       " ('2014-01-07', 13.083333333333334, 158),\n",
       " ('2014-01-02', 12.727272727272727, 130),\n",
       " ('2014-02-13', 12.226415094339623, 135),\n",
       " ('2014-03-26', 11.954545454545455, 191),\n",
       " ('2014-03-29', 11.914893617021276, 207),\n",
       " ('2014-12-07', 11.6, 153),\n",
       " ('2014-12-09', 11.26923076923077, 177),\n",
       " ('2014-12-08', 11.266666666666667, 169),\n",
       " ('2014-11-02', 10.826086956521738, 171)]"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(QUERY_5).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "029bda39",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_5, QUERY_5_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf426cd",
   "metadata": {},
   "source": [
    "### Query 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1606f740",
   "metadata": {},
   "source": [
    "#### During Hurricane Sandy in NYC (Oct 29-30, 2012), plus the week leading up and the week after, how many trips were taken each hour, and for each hour, how much precipitation did NYC receive and what was the hourly wind speed? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "07041a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_6_FILENAME = \"Hurricane_Sandy_trips\"\n",
    "\n",
    "QUERY_6 = \"\"\"\n",
    "WITH hired_trips AS (\n",
    "    SELECT pickup_datetime \n",
    "    FROM taxi_trips\n",
    "    WHERE pickup_datetime BETWEEN '2012-10-22' AND '2012-11-06'\n",
    "    UNION ALL\n",
    "    SELECT pickup_datetime\n",
    "    FROM uber_trips\n",
    "    WHERE pickup_datetime BETWEEN '2012-10-22' AND '2012-11-06'\n",
    "    )\n",
    "SELECT \n",
    "    strftime('%Y-%m-%d %H', hourly_weather.date) as hour_in_day,\n",
    "    COALESCE(COUNT(*), 0),\n",
    "    hourly_weather.hourly_precipitation,\n",
    "    hourly_weather.hourly_wind_speed\n",
    "FROM hourly_weather\n",
    "LEFT JOIN hired_trips\n",
    "ON \n",
    "    strftime('%Y-%m-%d %H', hourly_weather.date) = strftime('%Y-%m-%d %H', hired_trips.pickup_datetime)\n",
    "WHERE \n",
    "    hour_in_day BETWEEN '2012-10-22' AND '2012-11-06'\n",
    "GROUP BY \n",
    "    hour_in_day\n",
    "ORDER BY \n",
    "    hour_in_day ASC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "57745a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2012-10-22 00', 2, 0.0, 7.0),\n",
       " ('2012-10-22 01', 1, 0.0, 5.0),\n",
       " ('2012-10-22 02', 2, 0.0, 7.0),\n",
       " ('2012-10-22 03', 1, 0.0, 0.0),\n",
       " ('2012-10-22 04', 1, 0.0, 0.0),\n",
       " ('2012-10-22 05', 2, 0.0, 0.0),\n",
       " ('2012-10-22 06', 4, 0.0, 5.0),\n",
       " ('2012-10-22 07', 7, 0.0, 3.0),\n",
       " ('2012-10-22 08', 9, 0.0, 3.0),\n",
       " ('2012-10-22 09', 12, 0.0, 5.0),\n",
       " ('2012-10-22 12', 6, 0.0, 11.0),\n",
       " ('2012-10-22 14', 6, 0.0, 7.0),\n",
       " ('2012-10-22 15', 8, 0.0, 6.0),\n",
       " ('2012-10-22 16', 8, 0.0, 3.0),\n",
       " ('2012-10-22 17', 8, 0.0, 7.0),\n",
       " ('2012-10-22 18', 13, 0.0, 5.0),\n",
       " ('2012-10-22 19', 6, 0.0, 5.0),\n",
       " ('2012-10-22 20', 14, 0.0, 3.0),\n",
       " ('2012-10-22 21', 8, 0.0, 0.0),\n",
       " ('2012-10-22 22', 14, 0.0, 3.0),\n",
       " ('2012-10-22 23', 5, 0.0, 3.0),\n",
       " ('2012-10-23 00', 5, 0.0, 3.0),\n",
       " ('2012-10-23 01', 2, 0.0, 0.0),\n",
       " ('2012-10-23 02', 1, 0.0, 3.0),\n",
       " ('2012-10-23 03', 1, 0.0, 0.0),\n",
       " ('2012-10-23 04', 1, 0.0, 3.0),\n",
       " ('2012-10-23 05', 2, 0.0, 0.0),\n",
       " ('2012-10-23 06', 5, 0.0, 0.0),\n",
       " ('2012-10-23 07', 13, 0.0, 0.0),\n",
       " ('2012-10-23 08', 11, 0.0, 0.0),\n",
       " ('2012-10-23 09', 7, 0.0, 3.0),\n",
       " ('2012-10-23 10', 12, 0.0, 0.0),\n",
       " ('2012-10-23 11', 9, 0.0, 3.0),\n",
       " ('2012-10-23 12', 4, 0.0, 0.0),\n",
       " ('2012-10-23 16', 1, 0.0, 3.0),\n",
       " ('2012-10-23 18', 9, 0.0, 5.0),\n",
       " ('2012-10-23 19', 8, 0.0, 0.0),\n",
       " ('2012-10-23 20', 14, 0.02, 0.0),\n",
       " ('2012-10-23 21', 9, 0.0, 5.0),\n",
       " ('2012-10-23 22', 11, 0.01, 0.0),\n",
       " ('2012-10-23 23', 8, 0.0, 5.0),\n",
       " ('2012-10-24 00', 6, 0.0, 3.0),\n",
       " ('2012-10-24 01', 2, 0.0, 6.0),\n",
       " ('2012-10-24 02', 20, 0.0, 5.0),\n",
       " ('2012-10-24 03', 1, 0.0, 7.0),\n",
       " ('2012-10-24 04', 1, 0.0, 7.0),\n",
       " ('2012-10-24 05', 2, 0.0, 6.0),\n",
       " ('2012-10-24 06', 4, 0.0, 5.0),\n",
       " ('2012-10-24 07', 15, 0.0, 5.0),\n",
       " ('2012-10-24 08', 7, 0.0, 0.0),\n",
       " ('2012-10-24 09', 22, 0.0, 0.0),\n",
       " ('2012-10-24 10', 4, 0.0, 7.0),\n",
       " ('2012-10-24 11', 5, 0.0, 7.0),\n",
       " ('2012-10-24 12', 12, 0.0, 8.0),\n",
       " ('2012-10-24 13', 8, 0.0, 8.0),\n",
       " ('2012-10-24 14', 18, 0.0, 6.0),\n",
       " ('2012-10-24 15', 8, 0.0, 7.0),\n",
       " ('2012-10-24 16', 6, 0.0, 8.0),\n",
       " ('2012-10-24 17', 8, 0.0, 5.0),\n",
       " ('2012-10-24 18', 9, 0.0, 7.0),\n",
       " ('2012-10-24 19', 21, 0.0, 8.0),\n",
       " ('2012-10-24 20', 22, 0.0, 0.0),\n",
       " ('2012-10-24 21', 26, 0.0, 3.0),\n",
       " ('2012-10-24 22', 20, 0.0, 5.0),\n",
       " ('2012-10-24 23', 8, 0.0, 0.0),\n",
       " ('2012-10-25 00', 14, 0.0, 6.0),\n",
       " ('2012-10-25 01', 2, 0.0, 3.0),\n",
       " ('2012-10-25 02', 8, 0.0, 3.0),\n",
       " ('2012-10-25 03', 2, 0.0, 6.0),\n",
       " ('2012-10-25 04', 1, 0.0, 6.0),\n",
       " ('2012-10-25 05', 2, 0.0, 0.0),\n",
       " ('2012-10-25 06', 8, 0.0, 5.0),\n",
       " ('2012-10-25 07', 5, 0.0, 6.0),\n",
       " ('2012-10-25 08', 8, 0.0, 5.0),\n",
       " ('2012-10-25 09', 4, 0.0, 3.0),\n",
       " ('2012-10-25 10', 9, 0.0, 6.0),\n",
       " ('2012-10-25 11', 7, 0.0, 0.0),\n",
       " ('2012-10-25 12', 7, 0.0, 6.0),\n",
       " ('2012-10-25 13', 7, 0.0, 0.0),\n",
       " ('2012-10-25 14', 10, 0.0, 5.0),\n",
       " ('2012-10-25 15', 8, 0.0, 5.0),\n",
       " ('2012-10-25 16', 7, 0.0, 0.0),\n",
       " ('2012-10-25 17', 11, 0.0, 3.0),\n",
       " ('2012-10-25 18', 6, 0.0, 0.0),\n",
       " ('2012-10-25 19', 7, 0.0, 0.0),\n",
       " ('2012-10-25 20', 18, 0.0, 3.0),\n",
       " ('2012-10-25 21', 17, 0.0, 3.0),\n",
       " ('2012-10-25 22', 11, 0.0, 3.0),\n",
       " ('2012-10-25 23', 7, 0.0, 0.0),\n",
       " ('2012-10-26 00', 12, 0.0, 0.0),\n",
       " ('2012-10-26 01', 4, 0.0, 0.0),\n",
       " ('2012-10-26 02', 4, 0.0, 0.0),\n",
       " ('2012-10-26 03', 3, 0.0, 3.0),\n",
       " ('2012-10-26 04', 3, 0.0, 0.0),\n",
       " ('2012-10-26 05', 1, 0.0, 0.0),\n",
       " ('2012-10-26 06', 4, 0.0, 0.0),\n",
       " ('2012-10-26 07', 9, 0.0, 3.0),\n",
       " ('2012-10-26 08', 1, 0.0, 3.0),\n",
       " ('2012-10-26 09', 12, 0.0, 3.0),\n",
       " ('2012-10-26 10', 6, 0.0, 3.0),\n",
       " ('2012-10-26 11', 8, 0.0, 3.0),\n",
       " ('2012-10-26 12', 6, 0.0, 0.0),\n",
       " ('2012-10-26 13', 10, 0.0, 3.0),\n",
       " ('2012-10-26 14', 6, 0.0, 3.0),\n",
       " ('2012-10-26 15', 3, 0.0, 0.0),\n",
       " ('2012-10-26 16', 4, 0.0, 0.0),\n",
       " ('2012-10-26 17', 11, 0.0, 0.0),\n",
       " ('2012-10-26 18', 6, 0.0, 0.0),\n",
       " ('2012-10-26 19', 8, 0.0, 0.0),\n",
       " ('2012-10-26 20', 14, 0.0, 3.0),\n",
       " ('2012-10-26 21', 8, 0.0, 3.0),\n",
       " ('2012-10-26 22', 15, 0.0, 0.0),\n",
       " ('2012-10-26 23', 8, 0.0, 0.0),\n",
       " ('2012-10-27 00', 10, 0.0, 3.0),\n",
       " ('2012-10-27 01', 4, 0.0, 0.0),\n",
       " ('2012-10-27 02', 14, 0.0, 3.0),\n",
       " ('2012-10-27 03', 6, 0.0, 0.0),\n",
       " ('2012-10-27 04', 2, 0.0, 6.0),\n",
       " ('2012-10-27 05', 4, 0.0, 6.0),\n",
       " ('2012-10-27 06', 2, 0.0, 6.0),\n",
       " ('2012-10-27 07', 4, 0.0, 5.0),\n",
       " ('2012-10-27 08', 9, 0.0, 5.0),\n",
       " ('2012-10-27 09', 10, 0.0, 6.0),\n",
       " ('2012-10-27 10', 6, 0.0, 7.0),\n",
       " ('2012-10-27 11', 28, 0.0, 5.0),\n",
       " ('2012-10-27 12', 7, 0.0, 8.0),\n",
       " ('2012-10-27 13', 7, 0.0, 8.0),\n",
       " ('2012-10-27 14', 8, 0.0, 10.0),\n",
       " ('2012-10-27 15', 4, 0.0, 10.0),\n",
       " ('2012-10-27 16', 8, 0.0, 7.0),\n",
       " ('2012-10-27 17', 15, 0.0, 7.0),\n",
       " ('2012-10-27 18', 9, 0.0, 7.0),\n",
       " ('2012-10-27 19', 30, 0.0, 8.0),\n",
       " ('2012-10-27 20', 15, 0.0, 7.0),\n",
       " ('2012-10-27 21', 12, 0.0, 9.0),\n",
       " ('2012-10-27 22', 17, 0.0, 9.0),\n",
       " ('2012-10-27 23', 18, 0.0, 8.0),\n",
       " ('2012-10-28 00', 14, 0.0, 11.0),\n",
       " ('2012-10-28 01', 15, 0.0, 8.0),\n",
       " ('2012-10-28 02', 9, 0.0, 8.0),\n",
       " ('2012-10-28 03', 7, 0.0, 9.0),\n",
       " ('2012-10-28 04', 7, 0.0, 10.0),\n",
       " ('2012-10-28 05', 2, 0.0, 11.0),\n",
       " ('2012-10-28 06', 4, 0.0, 10.0),\n",
       " ('2012-10-28 07', 4, 0.0, 11.0),\n",
       " ('2012-10-28 08', 3, 0.0, 11.0),\n",
       " ('2012-10-28 09', 9, 0.0, 11.0),\n",
       " ('2012-10-28 10', 4, 0.0, 10.0),\n",
       " ('2012-10-28 11', 10, 0.0, 8.0),\n",
       " ('2012-10-28 12', 4, 0.0, 7.0),\n",
       " ('2012-10-28 13', 4, 0.0, 13.0),\n",
       " ('2012-10-28 14', 9, 0.0, 13.0),\n",
       " ('2012-10-28 15', 10, 0.0, 13.0),\n",
       " ('2012-10-28 16', 6, 0.0, 16.0),\n",
       " ('2012-10-28 17', 8, 0.0, 11.0),\n",
       " ('2012-10-28 18', 6, 0.0, 15.0),\n",
       " ('2012-10-28 19', 4, 0.0, 14.0),\n",
       " ('2012-10-28 20', 9, 0.0, 16.0),\n",
       " ('2012-10-28 21', 5, 0.0, 14.0),\n",
       " ('2012-10-28 22', 3, 0.0, 16.0),\n",
       " ('2012-10-28 23', 2, 0.0, 14.0),\n",
       " ('2012-10-29 00', 2, 0.0, 16.0),\n",
       " ('2012-10-29 01', 1, 0.0, 11.0),\n",
       " ('2012-10-29 02', 2, 0.0, 13.0),\n",
       " ('2012-10-29 03', 1, 0.0, 17.0),\n",
       " ('2012-10-29 04', 1, 0.0, 15.0),\n",
       " ('2012-10-29 05', 1, 0.0, 15.0),\n",
       " ('2012-10-29 06', 1, 0.02, 16.0),\n",
       " ('2012-10-29 07', 1, 0.02, 17.0),\n",
       " ('2012-10-29 08', 1, 0.0, 21.0),\n",
       " ('2012-10-29 09', 2, 0.0, 16.0),\n",
       " ('2012-10-29 11', 28, 0.0, 21.0),\n",
       " ('2012-10-29 12', 25, 0.02, 15.0),\n",
       " ('2012-10-29 13', 3, 0.02, 24.0),\n",
       " ('2012-10-29 14', 16, 0.03, 23.0),\n",
       " ('2012-10-29 15', 6, 0.07, 26.0),\n",
       " ('2012-10-29 16', 3, 0.1, 23.0),\n",
       " ('2012-10-29 17', 8, 0.04, 29.0),\n",
       " ('2012-10-29 18', 6, 0.02, 21.0),\n",
       " ('2012-10-29 19', 1, 0.01, 25.0),\n",
       " ('2012-10-29 20', 1, 0.0, 17.0),\n",
       " ('2012-10-29 21', 1, 0.0, 15.0),\n",
       " ('2012-10-29 22', 1, 0.02, 9.0),\n",
       " ('2012-10-29 23', 4, 0.03, 7.0),\n",
       " ('2012-10-30 00', 2, 0.03, 13.0),\n",
       " ('2012-10-30 01', 3, 0.0, 13.0),\n",
       " ('2012-10-30 02', 1, 0.03, 9.0),\n",
       " ('2012-10-30 03', 1, 0.04, 17.0),\n",
       " ('2012-10-30 04', 1, 0.0, 9.0),\n",
       " ('2012-10-30 05', 1, 0.01, 7.0),\n",
       " ('2012-10-30 06', 1, 0.01, 7.0),\n",
       " ('2012-10-30 07', 1, 0.0, 10.0),\n",
       " ('2012-10-30 08', 3, 0.01, 11.0),\n",
       " ('2012-10-30 09', 9, 0.01, 15.0),\n",
       " ('2012-10-30 10', 15, 0.02, 8.0),\n",
       " ('2012-10-30 11', 7, 0.0, 7.0),\n",
       " ('2012-10-30 12', 6, 0.0, 9.0),\n",
       " ('2012-10-30 13', 12, 0.0, 7.0),\n",
       " ('2012-10-30 16', 15, 0.01, 3.0),\n",
       " ('2012-10-30 17', 12, 0.0, 6.0),\n",
       " ('2012-10-30 18', 6, 0.0, 5.0),\n",
       " ('2012-10-30 19', 7, 0.0, 3.0),\n",
       " ('2012-10-30 20', 8, 0.0, 0.0),\n",
       " ('2012-10-30 21', 5, 0.0, 5.0),\n",
       " ('2012-10-30 22', 4, 0.0, 7.0),\n",
       " ('2012-10-30 23', 4, 0.0, 5.0),\n",
       " ('2012-10-31 00', 3, 0.0, 3.0),\n",
       " ('2012-10-31 01', 1, 0.01, 5.0),\n",
       " ('2012-10-31 02', 4, 0.0, 0.0),\n",
       " ('2012-10-31 03', 1, 0.0, 8.0),\n",
       " ('2012-10-31 05', 1, 0.0, 0.0),\n",
       " ('2012-10-31 06', 5, 0.0, 6.0),\n",
       " ('2012-10-31 07', 7, 0.0, 8.0),\n",
       " ('2012-10-31 08', 6, 0.0, 6.0),\n",
       " ('2012-10-31 09', 10, 0.0, 6.0),\n",
       " ('2012-10-31 10', 4, 0.0, 5.0),\n",
       " ('2012-10-31 11', 7, 0.0, 5.0),\n",
       " ('2012-10-31 12', 2, 0.0, 9.0),\n",
       " ('2012-10-31 13', 5, 0.0, 6.0),\n",
       " ('2012-10-31 14', 7, 0.0, 5.0),\n",
       " ('2012-10-31 15', 4, 0.0, 3.0),\n",
       " ('2012-10-31 16', 3, 0.0, 5.0),\n",
       " ('2012-10-31 17', 3, 0.0, 5.0),\n",
       " ('2012-10-31 18', 12, 0.0, 3.0),\n",
       " ('2012-10-31 19', 3, 0.0, 9.0),\n",
       " ('2012-10-31 20', 5, 0.0, 7.0),\n",
       " ('2012-10-31 21', 3, 0.0, 7.0),\n",
       " ('2012-10-31 22', 8, 0.0, 6.0),\n",
       " ('2012-10-31 23', 2, 0.0, 3.0),\n",
       " ('2012-11-01 00', 7, 0.0, 3.0),\n",
       " ('2012-11-01 01', 3, 0.0, 3.0),\n",
       " ('2012-11-01 02', 2, 0.0, 3.0),\n",
       " ('2012-11-01 04', 2, 0.0, 7.0),\n",
       " ('2012-11-01 05', 1, 0.0, 6.0),\n",
       " ('2012-11-01 06', 3, 0.0, 13.0),\n",
       " ('2012-11-01 08', 7, 0.0, 7.0),\n",
       " ('2012-11-01 09', 3, 0.0, 3.0),\n",
       " ('2012-11-01 10', 10, 0.0, 6.0),\n",
       " ('2012-11-01 11', 8, 0.0, 6.0),\n",
       " ('2012-11-01 12', 7, 0.0, 11.0),\n",
       " ('2012-11-01 13', 4, 0.0, 8.0),\n",
       " ('2012-11-01 14', 8, 0.0, 8.0),\n",
       " ('2012-11-01 16', 3, 0.0, 5.0),\n",
       " ('2012-11-01 17', 4, 0.0, 5.0),\n",
       " ('2012-11-01 18', 13, 0.0, 9.0),\n",
       " ('2012-11-01 19', 11, 0.0, 3.0),\n",
       " ('2012-11-01 20', 7, 0.0, 5.0),\n",
       " ('2012-11-01 21', 2, 0.0, 8.0),\n",
       " ('2012-11-01 22', 10, 0.0, 5.0),\n",
       " ('2012-11-01 23', 4, 0.0, 0.0),\n",
       " ('2012-11-02 00', 6, 0.0, 5.0),\n",
       " ('2012-11-02 01', 1, 0.0, 7.0),\n",
       " ('2012-11-02 02', 2, 0.0, 3.0),\n",
       " ('2012-11-02 03', 2, 0.0, 3.0),\n",
       " ('2012-11-02 04', 1, 0.0, 5.0),\n",
       " ('2012-11-02 05', 1, 0.0, 5.0),\n",
       " ('2012-11-02 06', 3, 0.0, 6.0),\n",
       " ('2012-11-02 08', 6, 0.0, 5.0),\n",
       " ('2012-11-02 09', 1, 0.0, 7.0),\n",
       " ('2012-11-02 10', 4, 0.0, 9.0),\n",
       " ('2012-11-02 11', 3, 0.0, 7.0),\n",
       " ('2012-11-02 12', 6, 0.0, 7.0),\n",
       " ('2012-11-02 13', 3, 0.0, 6.0),\n",
       " ('2012-11-02 14', 7, 0.0, 6.0),\n",
       " ('2012-11-02 15', 7, 0.0, 5.0),\n",
       " ('2012-11-02 16', 6, 0.0, 11.0),\n",
       " ('2012-11-02 17', 3, 0.0, 8.0),\n",
       " ('2012-11-02 18', 4, 0.0, 9.0),\n",
       " ('2012-11-02 19', 8, 0.0, 7.0),\n",
       " ('2012-11-02 20', 13, 0.0, 9.0),\n",
       " ('2012-11-02 21', 12, 0.0, 7.0),\n",
       " ('2012-11-02 22', 7, 0.0, 8.0),\n",
       " ('2012-11-02 23', 6, 0.0, 8.0),\n",
       " ('2012-11-03 00', 8, 0.0, 7.0),\n",
       " ('2012-11-03 01', 5, 0.0, 7.0),\n",
       " ('2012-11-03 02', 8, 0.0, 7.0),\n",
       " ('2012-11-03 03', 1, 0.0, 7.0),\n",
       " ('2012-11-03 04', 2, 0.0, 8.0),\n",
       " ('2012-11-03 05', 1, 0.0, 8.0),\n",
       " ('2012-11-03 06', 1, 0.0, 7.0),\n",
       " ('2012-11-03 07', 2, 0.0, 6.0),\n",
       " ('2012-11-03 08', 1, 0.0, 10.0),\n",
       " ('2012-11-03 09', 6, 0.0, 13.0),\n",
       " ('2012-11-03 10', 5, 0.0, 6.0),\n",
       " ('2012-11-03 11', 10, 0.0, 13.0),\n",
       " ('2012-11-03 12', 9, 0.0, 13.0),\n",
       " ('2012-11-03 13', 7, 0.0, 8.0),\n",
       " ('2012-11-03 14', 14, 0.0, 8.0),\n",
       " ('2012-11-03 15', 4, 0.0, 7.0),\n",
       " ('2012-11-03 16', 5, 0.0, 10.0),\n",
       " ('2012-11-03 17', 14, 0.0, 9.0),\n",
       " ('2012-11-03 18', 7, 0.0, 9.0),\n",
       " ('2012-11-03 19', 12, 0.0, 13.0),\n",
       " ('2012-11-03 20', 10, 0.0, 10.0),\n",
       " ('2012-11-03 21', 15, 0.0, 9.0),\n",
       " ('2012-11-03 22', 9, 0.0, 0.0),\n",
       " ('2012-11-03 23', 14, 0.0, 7.0),\n",
       " ('2012-11-04 00', 9, 0.0, 9.0),\n",
       " ('2012-11-04 01', 20, 0.0, 7.0),\n",
       " ('2012-11-04 02', 11, 0.0, 7.0),\n",
       " ('2012-11-04 03', 1, 0.0, 7.0),\n",
       " ('2012-11-04 04', 3, 0.0, 8.0),\n",
       " ('2012-11-04 05', 1, 0.0, 6.0),\n",
       " ('2012-11-04 06', 2, 0.0, 6.0),\n",
       " ('2012-11-04 07', 2, 0.0, 3.0),\n",
       " ('2012-11-04 08', 4, 0.0, 7.0),\n",
       " ('2012-11-04 09', 3, 0.0, 9.0),\n",
       " ('2012-11-04 11', 7, 0.0, 6.0),\n",
       " ('2012-11-04 12', 7, 0.0, 8.0),\n",
       " ('2012-11-04 13', 5, 0.0, 8.0),\n",
       " ('2012-11-04 14', 5, 0.0, 7.0),\n",
       " ('2012-11-04 15', 4, 0.0, 7.0),\n",
       " ('2012-11-04 16', 6, 0.0, 5.0),\n",
       " ('2012-11-04 17', 11, 0.0, 5.0),\n",
       " ('2012-11-04 19', 11, 0.0, 7.0),\n",
       " ('2012-11-04 21', 3, 0.0, 7.0),\n",
       " ('2012-11-04 22', 6, 0.0, 6.0),\n",
       " ('2012-11-04 23', 4, 0.0, 5.0),\n",
       " ('2012-11-05 00', 3, 0.0, 0.0),\n",
       " ('2012-11-05 01', 1, 0.0, 5.0),\n",
       " ('2012-11-05 02', 1, 0.0, 3.0),\n",
       " ('2012-11-05 03', 1, 0.0, 7.0),\n",
       " ('2012-11-05 04', 1, 0.0, 3.0),\n",
       " ('2012-11-05 05', 1, 0.0, 6.0),\n",
       " ('2012-11-05 06', 6, 0.0, 8.0),\n",
       " ('2012-11-05 07', 4, 0.0, 6.0),\n",
       " ('2012-11-05 08', 8, 0.0, 7.0),\n",
       " ('2012-11-05 09', 9, 0.0, 3.0),\n",
       " ('2012-11-05 10', 8, 0.0, 3.0),\n",
       " ('2012-11-05 11', 5, 0.0, 3.0),\n",
       " ('2012-11-05 12', 7, 0.0, 5.0),\n",
       " ('2012-11-05 13', 8, 0.0, 3.0),\n",
       " ('2012-11-05 15', 6, 0.0, 8.0),\n",
       " ('2012-11-05 17', 19, 0.0, 5.0),\n",
       " ('2012-11-05 18', 13, 0.0, 5.0),\n",
       " ('2012-11-05 19', 10, 0.0, 0.0),\n",
       " ('2012-11-05 20', 11, 0.0, 3.0),\n",
       " ('2012-11-05 21', 12, 0.0, 7.0),\n",
       " ('2012-11-05 22', 7, 0.0, 6.0),\n",
       " ('2012-11-05 23', 2, 0.0, 9.0)]"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(QUERY_6).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "e6fba88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only 340???\n",
    "len(engine.execute(QUERY_6).fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "ca9868d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_6, QUERY_6_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
