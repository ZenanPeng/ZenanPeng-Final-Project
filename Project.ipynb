{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1uAUJGEUzfNj6OsWNAimnYCw7eKaHhMUfU1MTj9YwYw4/edit?usp=sharing), [grading rubric](https://docs.google.com/document/d/1hKuRWqFcIdhOkow3Nljcm7PXzIkoa9c_aHkMKZDxWa0/edit?usp=sharing)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only an outline to help you with your own approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import math\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need; some have been added for you, and \n",
    "# some you need to fill in\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"./taxi_zones\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "UBER_CSV = \"uber_rides_sample.csv\"\n",
    "WEATHER_CSV_DIR = \"\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d53e24",
   "metadata": {},
   "source": [
    "### Load Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58708809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_zones():\n",
    "    '''\n",
    "    This function is used for loading the taxi_zones.shp file and cleaning the file,\n",
    "    including adding longitude and latitude columns by looking up the polygon,\n",
    "    removing the duplicate LocationID, setting a new index, and removing useless columns\n",
    "    '''\n",
    "    \n",
    "    #load taxi_zones shapefile\n",
    "    taxi_zones_shapefile = gpd.read_file('./taxi_zones.shp')\n",
    "    shp = taxi_zones_shapefile.to_crs(4326)\n",
    "    \n",
    "    #look up and get longitude column\n",
    "    shp[\"longitude\"]=shp.centroid.x\n",
    "    #look up and get latitude column\n",
    "    shp[\"latitude\"]=shp.centroid.y\n",
    "    \n",
    "    #check if there are any duplicate values in LocationID \n",
    "    if shp.loc[shp['OBJECTID'] != shp['LocationID']]['LocationID'].count() != 0:\n",
    "        #manually correct the duplicate values of LocationID\n",
    "        shp.loc[shp['OBJECTID'] == 57, 'LocationID'] = 57\n",
    "        shp.loc[shp['OBJECTID'] == 104, 'LocationID'] = 104\n",
    "        shp.loc[shp['OBJECTID'] == 105, 'LocationID'] = 105\n",
    "    \n",
    "    #check if the index is 'OBJECTID'\n",
    "    if shp.index.name != 'OBJECTID':\n",
    "        #change the index to 'OBJECTID'\n",
    "        shp = shp.set_index('OBJECTID')\n",
    "    \n",
    "    #only keep useful columns\n",
    "    shp = shp[['LocationID','longitude','latitude']]\n",
    "    \n",
    "    #after cleaning, return the shp file\n",
    "    return shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "576c1984",
   "metadata": {},
   "outputs": [
    {
     "ename": "DriverError",
     "evalue": "./taxi_zones.shp: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32mfiona/ogrext.pyx\u001b[0m in \u001b[0;36mfiona.ogrext.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfiona/_err.pyx\u001b[0m in \u001b[0;36mfiona._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: ./taxi_zones.shp: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDriverError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_28273/2234547445.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_and_clean_taxi_zones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/ln/t3cf08b1653_3b58vkrg2_9c0000gn/T/ipykernel_28273/591166313.py\u001b[0m in \u001b[0;36mget_and_clean_taxi_zones\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#load taxi_zones shapefile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtaxi_zones_shapefile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./taxi_zones.shp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mshp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtaxi_zones_shapefile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_crs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4326\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/geopandas/io/file.py\u001b[0m in \u001b[0;36m_read_file\u001b[0;34m(filename, bbox, mask, rows, engine, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"fiona\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         return _read_file_fiona(\n\u001b[0m\u001b[1;32m    260\u001b[0m             \u001b[0mpath_or_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         )\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/geopandas/io/file.py\u001b[0m in \u001b[0;36m_read_file_fiona\u001b[0;34m(path_or_bytes, from_bytes, bbox, mask, rows, where, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfiona_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m             \u001b[0mcrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrs_wkt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0;31m# attempt to get EPSG code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/fiona/env.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0menv_ctor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/fiona/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, allow_unsupported_drivers, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             colxn = Collection(\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/fiona/collection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, include_fields, wkt_version, allow_unsupported_drivers, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWritingSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mfiona/ogrext.pyx\u001b[0m in \u001b[0;36mfiona.ogrext.Session.start\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfiona/ogrext.pyx\u001b[0m in \u001b[0;36mfiona.ogrext.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDriverError\u001b[0m: ./taxi_zones.shp: No such file or directory"
     ]
    }
   ],
   "source": [
    "get_and_clean_taxi_zones()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d04c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_coords_for_taxi_zone_id(zone_loc_id, loaded_taxi_zones):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_with_coords(from_coord, to_coord):\n",
    "    \n",
    "    R = 6371.0\n",
    "    \n",
    "    pickup_lat = math.radians(from_coord['pickup_latitude'])\n",
    "    pickup_lon = math.radians(from_coord['pickup_longitude'])\n",
    "    dropoff_lat = math.radians(to_coord['dropoff_latitude'])\n",
    "    dropoff_lon = math.radians(to_coord['dropoff_longitude'])\n",
    "    \n",
    "    diff_lon = dropoff_lon - pickup_lon\n",
    "    diff_lat = dropoff_lat - pickup_lat\n",
    "    \n",
    "    a = math.sin(diff_lat/2)**2 + math.cos(pickup_lat) * math.cos(dropoff_lat) * math.sin(diff_lon/2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_column(df):\n",
    "    \n",
    "    df['distance'] = df.apply(lambda x: calculate_distance_with_coords(\n",
    "                            x[['pickup_latitude','pickup_longitude']], \n",
    "                            x[['dropoff_latitude','dropoff_longitude']]), axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Process Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53ba008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_yellow_taxi_urls(TAXI_URL):\n",
    "    \"\"\"\n",
    "    This function, get_parquet_files(), is used for collecting all urls of \n",
    "    yellow taxi data from 2009-01 to 2015-06 and save them into a list.\n",
    "    \"\"\"\n",
    "    import re #using the re module\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    \n",
    "    response = requests.get(TAXI_URL)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    #write a regular expression to help pull out the desired links for Yellow Taxi Parquet files\n",
    "    pattern = r\"yellow_tripdata_(2009-(0[1-9]|1[0-2])|201[0-4]-(0[1-9]|1[0-2])|2015-(0[1-6]))\"\n",
    "    parquet_links_list = []\n",
    "    \n",
    "    for url in [a['href'] for a in soup.find_all('a')]:\n",
    "        if re.search(pattern, url):\n",
    "            parquet_links_list.append(url)\n",
    "            \n",
    "    return parquet_links_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month(taxi_data_url):\n",
    "    \n",
    "    #give an unique file name by slicing the corresponding month \n",
    "    filename = taxi_data_url.split('/')[-1] \n",
    "    \n",
    "    zones_file = get_and_clean_taxi_zones()\n",
    "    \n",
    "    #check if the file is already downloaded to the current path\n",
    "    if not os.path.exists(filename):\n",
    "        response = requests.get(taxi_data_url, stream=True)\n",
    "        #download the monthly parquet file from correspond taxi url\n",
    "        with open(filename, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024): \n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "    \n",
    "    \n",
    "    #clean the parquet file for corresponding month\n",
    "    if '2009' in filename:\n",
    "        \n",
    "        df = pd.read_parquet(filename)\n",
    "        \n",
    "        df = df[['Trip_Pickup_DateTime','Tip_Amt','Start_Lon','Start_Lat','End_Lon','End_Lat']]\n",
    "        \n",
    "        df.rename(columns={'Trip_Pickup_DateTime':'pickup_datetime','Tip_Amt':'tip_amount',\n",
    "                           'Start_Lon':'pickup_longitude','Start_Lat':'pickup_latitude',\n",
    "                           'End_Lon':'dropoff_longitude','End_Lat':'dropoff_latitude'}, inplace=True)\n",
    "    \n",
    "    elif '2010' in filename:\n",
    "        \n",
    "        df = pd.read_parquet(filename)\n",
    "        df = df[['pickup_datetime','tip_amount','pickup_longitude',\n",
    "                 'pickup_latitude','dropoff_longitude','dropoff_latitude']]\n",
    "    \n",
    "    elif '2011' in filename or '2012' in filename or '2013' in filename or '2014' in filename or '2015' in filename:\n",
    "        \n",
    "        df = pd.read_parquet(filename)\n",
    "        df = df[['tpep_pickup_datetime','PULocationID','DOLocationID','tip_amount']]\n",
    "        \n",
    "        #zones_file = get_and_clean_taxi_zones()\n",
    "        \n",
    "        df = pd.merge(df, zones_file, left_on=\"PULocationID\", right_on=\"LocationID\", how=\"left\")\n",
    "        df = pd.merge(df, zones_file, left_on=\"DOLocationID\", right_on=\"LocationID\", \n",
    "                             suffixes=(\"_PU\", \"_DO\"), how=\"left\")\n",
    "        \n",
    "        df = df[['tpep_pickup_datetime','tip_amount',\n",
    "                 'longitude_PU','latitude_PU','longitude_DO','latitude_DO']]\n",
    "        \n",
    "        df.rename(columns={'tpep_pickup_datetime':'pickup_datetime',\n",
    "                           'longitude_PU':'pickup_longitude','latitude_PU':'pickup_latitude',\n",
    "                           'longitude_DO':'dropoff_longitude','latitude_DO':'dropoff_latitude'},inplace=True)\n",
    "    \n",
    "    df = df[df['tip_amount']>=0]\n",
    "    \n",
    "    #removing all data outside of the New York Box range\n",
    "    \n",
    "    #for pickup_longitude and pickup_latitude\n",
    "    df = df[(df['pickup_longitude']>=NEW_YORK_BOX_COORDS[0][1])\n",
    "            &(df['pickup_longitude']<=NEW_YORK_BOX_COORDS[1][1])\n",
    "            &(df['pickup_latitude']>=NEW_YORK_BOX_COORDS[0][0])\n",
    "            &(df['pickup_latitude']<=NEW_YORK_BOX_COORDS[1][0])]\n",
    "    \n",
    "    #for dropoff_longitude and dropoff_latitude\n",
    "    df = df[(df['dropoff_longitude']>=NEW_YORK_BOX_COORDS[0][1])\n",
    "            &(df['dropoff_longitude']<=NEW_YORK_BOX_COORDS[1][1])\n",
    "            &(df['dropoff_latitude']>=NEW_YORK_BOX_COORDS[0][0])\n",
    "            &(df['dropoff_latitude']<=NEW_YORK_BOX_COORDS[1][0])]\n",
    "    \n",
    "    #remove all trips with the same pickup_longitude and dropoff_longitude\n",
    "    #and the same pickup_latitude and dropoff_latitude\n",
    "    #which may result distance = 0 \n",
    "    #filter out these values\n",
    "    df = df[(df['pickup_longitude']!=df['dropoff_longitude'])\n",
    "           &(df['pickup_latitude']!=df['dropoff_latitude'])]\n",
    "    \n",
    "    #change the type of \"pickup_datetime\" into python datetime\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "    \n",
    "    #make sure all the data types are the desired ones\n",
    "    df = df.astype({'tip_amount':'float64',\n",
    "                    'pickup_longitude':'float64','pickup_latitude':'float64',\n",
    "                    'dropoff_longitude':'float64','dropoff_latitude':'float64'})\n",
    "    \n",
    "    #since we need to match the amount of data from the taxi file with the one from the uber \n",
    "    #as we know that the cleaned uber data is about 195,000, \n",
    "    #we have total 78 months from 2009-01 to 2015-06, so 195,000/78 is about 2500 rows for each month\n",
    "    df = df.sample(2500)\n",
    "    \n",
    "    #return the cleaned taxi data frame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(parquet_urls):\n",
    "    \n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    for parquet_url in parquet_urls:\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        dataframe = get_and_clean_month(parquet_url)\n",
    "        add_distance_column(dataframe)\n",
    "        # maybe: if the file hasn't been saved, save it so you can\n",
    "        # avoid re-downloading it if you re-run the function\n",
    "        \n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.contact(all_taxi_dataframes)\n",
    "    \n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200776ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data():\n",
    "    \n",
    "    all_urls = get_all_yellow_taxi_urls(TAXI_URL)\n",
    "    taxi_data = get_and_clean_taxi_data(all_urls)\n",
    "    \n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876bd645",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data = get_taxi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebd75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data():\n",
    "    \n",
    "    #load in the uber csv file as a data frame by pandas, and set the first column as index\n",
    "    df_uber = pd.read_csv(UBER_CSV, index_col=0)\n",
    "    \n",
    "    #only keeping useful columns\n",
    "    df_uber = df_uber[['pickup_datetime','pickup_longitude','pickup_latitude',\n",
    "                       'dropoff_longitude','dropoff_latitude','passenger_count' ]]\n",
    "    \n",
    "    #removing the rows whose passenger count is less than or equal to 0, which is unpractical\n",
    "    df_uber = df_uber[df_uber['passenger_count']>0]\n",
    "    \n",
    "    #removing all data outside of the New York Box range\n",
    "    \n",
    "    #for pickup_longitude and pickup_latitude\n",
    "    df_uber = df_uber[(df_uber['pickup_longitude']>=NEW_YORK_BOX_COORDS[0][1])\n",
    "                  &(df_uber['pickup_longitude']<=NEW_YORK_BOX_COORDS[1][1])\n",
    "                  &(df_uber['pickup_latitude']>=NEW_YORK_BOX_COORDS[0][0])\n",
    "                  &(df_uber['pickup_latitude']<=NEW_YORK_BOX_COORDS[1][0])]\n",
    "    \n",
    "    #for dropoff_longitude and dropoff_latitude\n",
    "    df_uber = df_uber[(df_uber['dropoff_longitude']>=NEW_YORK_BOX_COORDS[0][1])\n",
    "                  &(df_uber['dropoff_longitude']<=NEW_YORK_BOX_COORDS[1][1])\n",
    "                  &(df_uber['dropoff_latitude']>=NEW_YORK_BOX_COORDS[0][0])\n",
    "                  &(df_uber['dropoff_latitude']<=NEW_YORK_BOX_COORDS[1][0])]\n",
    "    \n",
    "    #filter out distance = 0 \n",
    "    df_uber = df_uber[(df_uber['pickup_longitude']!=df_uber['dropoff_longitude'])\n",
    "                      &(df_uber['pickup_latitude']!=df_uber['dropoff_latitude'])]\n",
    "    \n",
    "    #change the type of \"pickup_datetime\" into python datetime\n",
    "    df_uber['pickup_datetime'] = pd.to_datetime(df_uber['pickup_datetime'])\n",
    "    \n",
    "    #change all the data types into desired ones\n",
    "    df_uber = df_uber.astype({'pickup_longitude':'float64','pickup_latitude':'float64',\n",
    "                              'dropoff_longitude':'float64','dropoff_latitude':'float64',\n",
    "                              'passenger_count':'int64'})\n",
    "    \n",
    "    #return the cleaned uber data frame\n",
    "    return df_uber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    uber_dataframe = load_and_clean_uber_data()\n",
    "    add_distance_column(uber_dataframe)\n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339997e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs():\n",
    "    \n",
    "    #initiate with an empty list \n",
    "    df_weathers = []\n",
    "    \n",
    "    #iterate the year from 2009 to 2015\n",
    "    for year in range(2009, 2016, 1):\n",
    "        df_weather = str(year) + '_weather.csv'\n",
    "        df_weathers.append(df_weather)\n",
    "    \n",
    "    #return a list with all weathers files names from 2009 to 2015\n",
    "    return df_weathers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_weather_data_hourly(csv_file):\n",
    "    \n",
    "    #load a certain weather csv file into data frame\n",
    "    df_weather = pd.read_csv(csv_file)\n",
    "    \n",
    "    #transform the data type of column 'Date' into a datetime format\n",
    "    df_weather['DATE'] = pd.to_datetime(df_weather['DATE'])\n",
    "    \n",
    "    #split hour and minute of a datetime and create corresponding columns respectively\n",
    "    df_weather['timestamp_hour'] = df_weather['DATE'].apply(lambda x:str(x.hour).zfill(2))\n",
    "    df_weather['timestamp_minute'] = df_weather['DATE'].apply(lambda x:str(x.minute).zfill(2))\n",
    "\n",
    "    #as I manually observe that the daily data is always collected at the last minute of that day\n",
    "    #I create a new data frame by filtering the datetime as the last minute of a day\n",
    "    df_weather_day_end = df_weather[(df_weather['timestamp_hour'] == '23')\n",
    "                                & (df_weather['timestamp_minute'] == '59')]\n",
    "    \n",
    "    #use the index to drop all daily collected weather data from the total weather data\n",
    "    #after that, we can get the pure hourly weather data \n",
    "    df_weather_hourly = df_weather.drop(df_weather_day_end.index)\n",
    "    \n",
    "    #reset the index\n",
    "    df_weather_hourly.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    #only keep the useful columns\n",
    "    df_weather_hourly = df_weather_hourly[['DATE','HourlyPrecipitation','HourlyWindSpeed']]\n",
    "    \n",
    "    # https://stackoverflow.com/questions/58807577/pandas-dataframe-extracting-float-values-from-string-in-a-column\n",
    "    #since I found that there are some data with units, strip all the units and only keep the numeric values\n",
    "    df_weather_hourly['HourlyPrecipitation'] = pd.to_numeric(\n",
    "        df_weather_hourly['HourlyPrecipitation'].str.extract(r'(\\d+\\.?\\d*)', expand=False), errors='coerce')\n",
    "    \n",
    "    #As the documentation mentions that the blank/null values indicate that\n",
    "    #no precipitation was observed/reported for the hour ending at that time\n",
    "    #also, the value of 'T' means trace amount of precipitation, \n",
    "    #therefore, we can directly fill all these values as 0\n",
    "    df_weather_hourly['HourlyPrecipitation'] = df_weather_hourly['HourlyPrecipitation'].fillna(0)\n",
    "    \n",
    "    #since there are some rows whose hourly wind speed is null\n",
    "    #as this is a very small amount, we can drop these values, which will not affect the whole trend\n",
    "    #by the law of large numbers\n",
    "    df_weather_hourly = df_weather_hourly.dropna(subset = 'HourlyWindSpeed')\n",
    "    \n",
    "    #make sure all the data types are the desired ones\n",
    "    df_weather_hourly = df_weather_hourly.astype({'HourlyPrecipitation':'float64',\n",
    "                                                  'HourlyWindSpeed':'float64'})\n",
    "    \n",
    "    #rename the essential columns\n",
    "    df_weather_hourly.rename(columns={'DATE':'Date'}, inplace=True)\n",
    "    \n",
    "    #make sure the 'Date' columns follow datetime format\n",
    "    df_weather_hourly['Date'] = pd.to_datetime(df_weather_hourly['Date'])\n",
    "    \n",
    "    #return the cleaned hourly weather data\n",
    "    return df_weather_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_weather_data_daily(csv_file):\n",
    "    \n",
    "    #load a certain weather csv file into data frame\n",
    "    df_weather = pd.read_csv(csv_file)\n",
    "    \n",
    "    #transform the data type of column 'Date' into a datetime format\n",
    "    df_weather['DATE'] = pd.to_datetime(df_weather['DATE'])\n",
    "    \n",
    "    #split hour and minute of a datetime and create corresponding columns respectively\n",
    "    df_weather['timestamp_hour'] = df_weather['DATE'].apply(lambda x:str(x.hour).zfill(2))\n",
    "    df_weather['timestamp_minute'] = df_weather['DATE'].apply(lambda x:str(x.minute).zfill(2))\n",
    "\n",
    "    #as I manually observe that the daily data is always collected at the last minute of that day\n",
    "    #I create a new data frame by filtering the datetime as the last minute of a day\n",
    "    df_weather_day_end = df_weather[(df_weather['timestamp_hour'] == '23')\n",
    "                                & (df_weather['timestamp_minute'] == '59')]\n",
    "    \n",
    "    #use the index to drop all daily collected weather data from the total weather data\n",
    "    #after that, we can get the pure hourly weather data \n",
    "    df_weather_hourly = df_weather.drop(df_weather_day_end.index)\n",
    "    \n",
    "    #rearrange the index, counting from zero and increment\n",
    "    df_weather_hourly.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    #only keep the useful columns\n",
    "    df_weather_hourly = df_weather_hourly[['DATE','HourlyPrecipitation','HourlyWindSpeed']]\n",
    "    \n",
    "    # https://stackoverflow.com/questions/58807577/pandas-dataframe-extracting-float-values-from-string-in-a-column\n",
    "    #found some data with units, clear all the units and only keep the numeric values\n",
    "    df_weather_hourly['HourlyPrecipitation'] = pd.to_numeric(\n",
    "        df_weather_hourly['HourlyPrecipitation'].str.extract(r'(\\d+\\.?\\d*)', expand=False), errors='coerce')\n",
    "    \n",
    "    #As the documentation mentions that the blank/null values indicate that\n",
    "    #no precipitation was observed/reported for the hour ending at that time\n",
    "    #also, the value of 'T' means trace amount of precipitation, \n",
    "    #therefore, we can directly fill all these values as 0\n",
    "    df_weather_hourly['HourlyPrecipitation'] = df_weather_hourly['HourlyPrecipitation'].fillna(0)\n",
    "    \n",
    "    #since there are some rows whose hourly wind speed is null\n",
    "    #as this is a very small amount, we can drop these values, which will not affect the whole trend\n",
    "    #by the law of large numbers\n",
    "    df_weather_hourly = df_weather_hourly.dropna(subset='HourlyWindSpeed')\n",
    "    \n",
    "    #since we can get mostly the hourly data from all weather files\n",
    "    #however, there are too many missing values in a daily frequency, especially from 2009 to 2012\n",
    "    #using hourly data to populate the daily data \n",
    "    #also, by observing the daily precipitation and daily average wind speed columns from 2013 to 2015\n",
    "    #the way to calculate the daily precipitation is to take summation\n",
    "    #the way to calculate the daily average wind speed is to take average of samples in a 24-hours interval\n",
    "    df_weather_daily = df_weather_hourly.groupby(\n",
    "        df_weather_hourly['DATE'].dt.date).agg({'HourlyPrecipitation': 'sum', 'HourlyWindSpeed': 'mean'})\n",
    "    \n",
    "    #since the index for daily df is the date, create another date column DATE with index values\n",
    "    df_weather_daily['DATE'] = df_weather_daily.index\n",
    "    \n",
    "    #make sure the DATE as the datetime format\n",
    "    df_weather_daily['DATE'] = pd.to_datetime(df_weather_daily['DATE'])\n",
    "    \n",
    "    #reset the default index as row number\n",
    "    df_weather_daily.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    #rename the essential columns\n",
    "    df_weather_daily.rename(columns={'DATE':'Date',\n",
    "                                     'HourlyPrecipitation':'DailyPrecipitation',\n",
    "                                     'HourlyWindSpeed':'DailyAverageWindSpeed'}, inplace=True)\n",
    "    \n",
    "    #make sure all the data types are the desired ones\n",
    "    df_weather_daily = df_weather_daily.astype({'DailyPrecipitation':'float64',\n",
    "                                                'DailyAverageWindSpeed':'float64'})\n",
    "    \n",
    "    #rearrange the order of all columns\n",
    "    df_weather_daily = df_weather_daily.reindex(columns=['Date','DailyPrecipitation','DailyAverageWindSpeed'])\n",
    "        \n",
    "    #return the cleaned version of daily weather data\n",
    "    return df_weather_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0094674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_sunrise_sunset_data(csv_file):\n",
    "    \n",
    "    #load a certain weather csv file into data frame\n",
    "    df_weather = pd.read_csv(csv_file)\n",
    "    \n",
    "    #transform the data type of column 'Date' into a datetime format\n",
    "    df_weather['DATE'] = pd.to_datetime(df_weather['DATE'])\n",
    "    \n",
    "    #split hour and minute of a datetime and create corresponding columns respectively\n",
    "    df_weather['timestamp_hour'] = df_weather['DATE'].apply(lambda x:str(x.hour).zfill(2))\n",
    "    df_weather['timestamp_minute'] = df_weather['DATE'].apply(lambda x:str(x.minute).zfill(2))\n",
    "\n",
    "    #as I manually observe that the daily data is always collected at the last minute of that day\n",
    "    #create a new data frame with only the daily data\n",
    "    df_weather_sun = df_weather[(df_weather['timestamp_hour'] == '23')\n",
    "                                & (df_weather['timestamp_minute'] == '59')]\n",
    "    \n",
    "    #rearrange the index, counting from zero and increment\n",
    "    df_weather_sun.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    #only keep the important columns\n",
    "    df_weather_sun = df_weather_sun[['DATE','Sunrise','Sunset']]\n",
    "    \n",
    "    #since we can't use any other columns to populate the null values\n",
    "    #directly drop the null values\n",
    "    df_weather_sun = df_weather_sun.dropna(subset=['Sunrise','Sunset'])\n",
    "    \n",
    "    #rename the essential columns\n",
    "    df_weather_sun.rename(columns={'DATE':'Date'}, inplace=True)\n",
    "    \n",
    "    #make sure the DATE as the datetime format\n",
    "    df_weather_sun['Date'] = pd.to_datetime(df_weather_sun['Date'])\n",
    "    \n",
    "    return df_weather_sun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    \n",
    "    weather_csv_files = get_all_weather_csvs()\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "    sun_dataframes = []\n",
    "        \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = get_and_clean_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = get_and_clean_weather_data_daily(csv_file)\n",
    "        sun_dataframe = get_and_clean_sunrise_sunset_data(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        sun_dataframes.append(sun_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every year\n",
    "    # create one dataframe with only sunrise and sunset data\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    sun_data = pd.concat(sun_dataframes)\n",
    "    \n",
    "    #make sure all index are counting the row number from 0 and gradually incrementing\n",
    "    hourly_data.reset_index(inplace=True, drop=True)\n",
    "    daily_data.reset_index(inplace=True, drop=True)\n",
    "    sun_data.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return hourly_data, daily_data, sun_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48216557",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather (\n",
    "    hourly_Id INTEGER PRIMARY KEY,\n",
    "    Date DATE,\n",
    "    HourlyPrecipitation FLOAT,\n",
    "    HourlyWindSpeed FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather (\n",
    "    daily_Id INTEGER PRIMARY KEY,\n",
    "    Date DATE,\n",
    "    DailyPrecipitation FLOAT,\n",
    "    DailyAverageWindSpeed FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips (\n",
    "    taxi_Id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    pickup_datetime DATE,\n",
    "    tip_amount FLOAT,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    distance FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips (\n",
    "    uber_Id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    pickup_datetime DATE,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    passenger_count INTEGER,\n",
    "    distance FLOAT\n",
    "    );\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_1).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
